{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doooooodlesssss/imagecap/blob/main/Copy_of_imgcap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lfr1lKzhZHZ",
        "outputId": "4fd5c12e-eb72-4d55-b774-65be54c7b1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFQfmRu4iaXy",
        "outputId": "f566a989-1827-4722-9a03-2990a67021c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<1.27 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.12/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: yolov4 in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "‚úÖ Patched YOLOv4 weights.py\n",
            "Call tf.config.experimental.set_memory_growth(GPU0, True)\n"
          ]
        }
      ],
      "source": [
        "# ‚ö†Ô∏è Restart runtime first, then run this cleanly:\n",
        "!pip install \"numpy<1.27\" opencv-python==4.6.0.66 yolov4\n",
        "\n",
        "# Patch the YOLOv4 weights.py\n",
        "file_path = \"/usr/local/lib/python3.12/dist-packages/yolov4/tf/utils/weights.py\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "code = code.replace(\n",
        "    \"conv_shape = (filters, conv2d.input_shape[-1], *conv2d.kernel_size)\",\n",
        "    \"conv_shape = (filters, conv2d.input.shape[-1], *conv2d.kernel_size)\"\n",
        ")\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"‚úÖ Patched YOLOv4 weights.py\")\n",
        "\n",
        "# Import YOLOv4 directly after patch\n",
        "from yolov4.tf import YOLOv4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQx3qAtZKM6F",
        "outputId": "b2112c4f-2596-49df-eb07-fe1184014126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1MRbY_LasFIeyhKUGSqG5utjF6qjx9rmq/image_captioning/image_captioning\n"
          ]
        }
      ],
      "source": [
        "repo_path = \"/content/image_captioning\"\n",
        "%cd /content/drive/MyDrive/image_captioning/image_captioning/\n",
        "\n",
        "kaggle_json = \"/content/drive/MyDrive/image_captioning/kaggle.json\"\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/image_captioning/datasets/coco2014\"\n",
        "\n",
        "feature_extraction_model = \"xception\"\n",
        "\n",
        "model_dir = \"trained_model_xception\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFLyFBzcKPzn",
        "outputId": "72884de3-5e04-4365-f3ce-a3a031abc530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow (preinstalled): 2.19.0\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "‚úîÔ∏è opencv-python-headless==4.6.0.66 already installed\n",
            "‚úîÔ∏è matplotlib already installed\n",
            "‚úîÔ∏è pillow already installed\n",
            "‚úîÔ∏è tqdm already installed\n",
            "‚úîÔ∏è pycocotools already installed\n",
            "‚úîÔ∏è lxml already installed\n",
            "‚úîÔ∏è pandas already installed\n",
            "‚úîÔ∏è seaborn already installed\n",
            "‚úîÔ∏è yolov4 already installed\n",
            "tensorflow already installed ‚úÖ\n",
            "kaggle already installed ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow (preinstalled):\", tf.__version__)\n",
        "import os, importlib.util\n",
        "import sys, pkgutil\n",
        "print(\"Python:\", sys.version)\n",
        "\n",
        "def safe_pip(pkg, import_name=None):\n",
        "    if import_name is None:\n",
        "        import_name = pkg.split(\"==\")[0]\n",
        "    if importlib.util.find_spec(import_name) is None:\n",
        "        !pip install {pkg}\n",
        "    else:\n",
        "        print(f\"‚úîÔ∏è {pkg} already installed\")\n",
        "\n",
        "safe_pip(\"opencv-python-headless==4.6.0.66\", \"cv2\")\n",
        "safe_pip(\"matplotlib\")\n",
        "safe_pip(\"pillow\", \"PIL\")\n",
        "safe_pip(\"tqdm\")\n",
        "safe_pip(\"pycocotools\")\n",
        "safe_pip(\"lxml\")\n",
        "safe_pip(\"pandas\")\n",
        "safe_pip(\"seaborn\")\n",
        "safe_pip(\"yolov4\")\n",
        "\n",
        "if importlib.util.find_spec(\"tensorflow\") is None:\n",
        "    !pip install tensorflow\n",
        "else:\n",
        "    print(\"tensorflow already installed ‚úÖ\")\n",
        "\n",
        "if importlib.util.find_spec(\"kaggle\") is None:\n",
        "    !pip install kaggle\n",
        "else:\n",
        "    print(\"kaggle already installed ‚úÖ\")\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import datetime\n",
        "import importlib\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "from builtins import len\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "import threading\n",
        "\n",
        "import cv2\n",
        "\n",
        "# import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0ZeK1zqeUvF"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "EMBEDDING_DIM = 256\n",
        "MAX_LENGTH = 40\n",
        "FEATURE_SHAPE = 2048   # CNN or YOLO feature dimension (flattened)\n",
        "\n",
        "WORD_DICT_SIZE = 15000\n",
        "LIMIT_SIZE = True\n",
        "EXAMPLE_NUMBER = 20000  # will only work if LIMIT_SIZE is True\n",
        "MY_EMBEDDING_DIM = 256\n",
        "UNIT_COUNT = 512\n",
        "MY_OPTIMIZER = tf.keras.optimizers.Adam()\n",
        "MY_LOSS_OBJECT = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "EPOCH_COUNT = 20\n",
        "REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN = True\n",
        "DATASET = \"mscoco\"  # \"mscoco\" or \"flickr8k\" or \"flickr30k\"\n",
        "TEST_SET_PROPORTION = 1\n",
        "feature_extraction_model = \"xception\"\n",
        "split = 0  # 0 for training, 1 for testing\n",
        "\n",
        "BATCH_SIZE = 32  # 64\n",
        "BUFFER_SIZE = 1000  # 1000\n",
        "embedding_dim = MY_EMBEDDING_DIM  # hadie\n",
        "units = UNIT_COUNT  # hadie\n",
        "UNITS = UNIT_COUNT  # hadie\n",
        "top_k = WORD_DICT_SIZE\n",
        "vocab_size = top_k + 1\n",
        "VOCAB_SIZE = top_k + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t27s38Q-czw4"
      },
      "outputs": [],
      "source": [
        "# Python program for implementation of Quicksort Sort\n",
        "def partition(arr, low, high):\n",
        "\ti = (low - 1)\n",
        "\tpivot = arr[high]\n",
        "\tfor j in range(low, high):\n",
        "\t\tif arr[j][6] >= pivot[6]:\n",
        "\t\t\ti = i + 1\n",
        "\t\t\tarr[i], arr[j] = arr[j], arr[i]\n",
        "\tarr[i + 1], arr[high] = arr[high], arr[i + 1]\n",
        "\treturn (i + 1)\n",
        "def quickSort(arr, low, high):\n",
        "\tif len(arr) == 1:\n",
        "\t\treturn arr\n",
        "\tif low < high:\n",
        "\t\tpi = partition(arr, low, high)\n",
        "\t\tquickSort(arr, low, pi - 1)\n",
        "\t\tquickSort(arr, pi + 1, high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_SlEbT2K5Ay"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"/content/drive/MyDrive/image_captioning/image_captioning\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class BahdanauAttention(tf.keras.Model):\n",
        "#     def __init__(self, units):\n",
        "#         super(BahdanauAttention, self).__init__()\n",
        "#         self.W1 = layers.Dense(units)\n",
        "#         self.W2 = layers.Dense(units)\n",
        "#         self.V = layers.Dense(1)\n",
        "\n",
        "#     def call(self, features, hidden):\n",
        "#         # features: (batch_size, seq_len, embed_dim)\n",
        "#         # hidden:   (batch_size, hidden_dim)\n",
        "#         hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "#         score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "#         attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "#         context_vector = attention_weights * features\n",
        "#         context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "#         return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_time = tf.expand_dims(hidden, 1)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_time))\n",
        "        attn_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "        context = attn_weights * features\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "        return context, attn_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "_cD02XRC-M1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models\n"
      ],
      "metadata": {
        "id": "l3IGQcSdRwQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# üîÆ Spectral Attention Block (FFT-based)\n",
        "# ====================================================\n",
        "# class SpectralAttention(tf.keras.Model):\n",
        "#     \"\"\"\n",
        "#     Applies learnable complex weighting in frequency domain.\n",
        "#     Input: (B, D)\n",
        "#     \"\"\"\n",
        "#     def __init__(self, feature_dim):\n",
        "#         super().__init__()\n",
        "#         # Learnable real and imaginary weights for each frequency\n",
        "#         self.real_weight = tf.Variable(\n",
        "#             tf.random.normal([feature_dim], stddev=0.02), trainable=True)\n",
        "#         self.imag_weight = tf.Variable(\n",
        "#             tf.random.normal([feature_dim], stddev=0.02), trainable=True)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         # x: (B, D)\n",
        "#         # 1D FFT along feature dimension\n",
        "#         x_complex = tf.cast(x, tf.complex64)\n",
        "#         freq = tf.signal.fft(x_complex)  # (B, D)\n",
        "\n",
        "#         # combine learnable complex weights\n",
        "#         spectral_weights = tf.complex(self.real_weight, self.imag_weight)\n",
        "#         freq_weighted = freq * spectral_weights  # elementwise complex multiply\n",
        "\n",
        "#         # inverse FFT to spatial domain\n",
        "#         x_enriched = tf.math.real(tf.signal.ifft(freq_weighted))  # (B, D)\n",
        "#         return x_enriched\n",
        "\n",
        "\n",
        "class SpectralAttention(tf.keras.Model):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.real_weight = tf.Variable(tf.random.normal([feature_dim], stddev=0.02))\n",
        "        self.imag_weight = tf.Variable(tf.random.normal([feature_dim], stddev=0.02))\n",
        "\n",
        "    def call(self, x):\n",
        "        x_complex = tf.cast(x, tf.complex64)\n",
        "        freq = tf.signal.fft(x_complex)\n",
        "        spectral_weights = tf.complex(self.real_weight, self.imag_weight)\n",
        "        weighted_freq = freq * spectral_weights\n",
        "        x_out = tf.math.real(tf.signal.ifft(weighted_freq))\n",
        "        return x_out\n",
        "\n"
      ],
      "metadata": {
        "id": "_60ZBpFXRH1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# ‚ö° Dual Attention Fusion + Spectral Attention\n",
        "# ====================================================\n",
        "# class DualSpectralFusion(tf.keras.Model):\n",
        "#     def __init__(self, units):\n",
        "#         super().__init__()\n",
        "#         self.att_yolo = BahdanauAttention(units)\n",
        "#         self.att_xcep = BahdanauAttention(units)\n",
        "#         self.spectral_att = SpectralAttention(units)\n",
        "\n",
        "#     def call(self, yolo_feats, xcep_feats, hidden):\n",
        "#         \"\"\"\n",
        "#         yolo_feats: (B, N_y, D)\n",
        "#         xcep_feats: (B, N_x, D)\n",
        "#         hidden:     (B, H)\n",
        "#         \"\"\"\n",
        "#         yolo_ctx, _ = self.att_yolo(yolo_feats, hidden)  # (B, D)\n",
        "#         xcep_ctx, _ = self.att_xcep(xcep_feats, hidden)  # (B, D)\n",
        "\n",
        "#         fused = yolo_ctx * xcep_ctx  # elementwise fusion (global interaction)\n",
        "#         enriched = self.spectral_att(fused)  # FFT attention\n",
        "\n",
        "#         return enriched  # (B, D)\n",
        "\n",
        "\n",
        "class DualSpectralFusion(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.att_yolo = BahdanauAttention(units)\n",
        "        self.att_xcep = BahdanauAttention(units)\n",
        "        self.spectral = SpectralAttention(units // 2)\n",
        "\n",
        "\n",
        "    def call(self, yolo_feats, xcep_feats, hidden):\n",
        "        yolo_ctx, _ = self.att_yolo(yolo_feats, hidden)\n",
        "        xcep_ctx, _ = self.att_xcep(xcep_feats, hidden)\n",
        "        fused = yolo_ctx * xcep_ctx\n",
        "        enriched = self.spectral(fused)\n",
        "        return enriched"
      ],
      "metadata": {
        "id": "ylZHpo_gRL9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# üéõÔ∏è DCT/IDCT Utils\n",
        "# =========================\n",
        "def dct_2d(x):\n",
        "    \"\"\"Apply 2D Discrete Cosine Transform.\"\"\"\n",
        "    x = tf.signal.dct(x, type=2, norm='ortho')\n",
        "    x = tf.signal.dct(tf.transpose(x, perm=[0, 2, 1]), type=2, norm='ortho')\n",
        "    return tf.transpose(x, perm=[0, 2, 1])\n",
        "\n",
        "def idct_2d(x):\n",
        "    \"\"\"Apply Inverse 2D DCT.\"\"\"\n",
        "    x = tf.signal.idct(x, type=2, norm='ortho')\n",
        "    x = tf.signal.idct(tf.transpose(x, perm=[0, 2, 1]), type=2, norm='ortho')\n",
        "    return tf.transpose(x, perm=[0, 2, 1])\n",
        "\n",
        "def keep_low_freq(x, keep_ratio=0.25):\n",
        "    \"\"\"Keep only low-frequency components.\"\"\"\n",
        "    B, H, W = x.shape\n",
        "    h_keep, w_keep = int(H * keep_ratio), int(W * keep_ratio)\n",
        "    mask = np.zeros((H, W))\n",
        "    mask[:h_keep, :w_keep] = 1.0\n",
        "    mask = tf.constant(mask, dtype=tf.float32)\n",
        "    return x * mask"
      ],
      "metadata": {
        "id": "AWhtvx4b-KuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# üß© Dual Attention Fusion Block\n",
        "# # =========================\n",
        "# class DualSpectralAttention(tf.keras.Model):\n",
        "#     def __init__(self, units):\n",
        "#         super(DualSpectralAttention, self).__init__()\n",
        "#         self.att_yolo = BahdanauAttention(units)\n",
        "#         self.att_xcep = BahdanauAttention(units)\n",
        "\n",
        "#     def call(self, yolo_feats, xcep_feats, hidden):\n",
        "#         \"\"\"\n",
        "#         yolo_feats: (batch_size, N_y, feature_dim)\n",
        "#         xcep_feats: (batch_size, N_x, feature_dim)\n",
        "#         hidden: (batch_size, hidden_dim)\n",
        "#         \"\"\"\n",
        "#         # Apply separate Bahdanau attention\n",
        "#         yolo_context, _ = self.att_yolo(yolo_feats, hidden)\n",
        "#         xcep_context, _ = self.att_xcep(xcep_feats, hidden)\n",
        "\n",
        "#         # Reshape to 2D (square-like) for DCT\n",
        "#         side_len = int(np.sqrt(yolo_context.shape[-1]))\n",
        "#         yolo_map = tf.reshape(yolo_context, (-1, side_len, side_len))\n",
        "#         xcep_map = tf.reshape(xcep_context, (-1, side_len, side_len))\n",
        "\n",
        "#         # Apply 2D DCT to both\n",
        "#         yolo_freq = dct_2d(yolo_map)\n",
        "#         xcep_freq = dct_2d(xcep_map)\n",
        "\n",
        "#         # Keep only low frequencies\n",
        "#         yolo_low = keep_low_freq(yolo_freq)\n",
        "#         xcep_low = keep_low_freq(xcep_freq)\n",
        "\n",
        "#         # Fuse via elementwise multiplication\n",
        "#         fused_freq = yolo_low * xcep_low\n",
        "\n",
        "#         # Return to spatial domain\n",
        "#         fused_map = idct_2d(fused_freq)\n",
        "\n",
        "#         # Flatten for decoder input\n",
        "#         fused_flat = tf.reshape(fused_map, (-1, fused_map.shape[-1] * fused_map.shape[-2]))\n",
        "\n",
        "#         return fused_flat\n"
      ],
      "metadata": {
        "id": "RimuSXp2-RfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# üß† Decoder with DualSpectralAttention\n",
        "# =========================\n",
        "# class RNN_Decoder(tf.keras.Model):\n",
        "#     def __init__(self, embedding_dim, units, vocab_size):\n",
        "#         super(RNN_Decoder, self).__init__()\n",
        "#         self.units = units\n",
        "#         self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
        "#         self.lstm = layers.LSTM(\n",
        "#             self.units, return_sequences=True, return_state=True,\n",
        "#             recurrent_initializer='glorot_uniform'\n",
        "#         )\n",
        "#         self.fc1 = layers.Dense(self.units)\n",
        "#         self.fc2 = layers.Dense(vocab_size)\n",
        "\n",
        "#         # Dual spectral attention block\n",
        "#         self.dual_att = DualSpectralAttention(units)\n",
        "\n",
        "#     def call(self, x, yolo_feats, xcep_feats, hidden):\n",
        "#         fused_context = self.dual_att(yolo_feats, xcep_feats, hidden)\n",
        "\n",
        "#         x = self.embedding(x)\n",
        "#         x = tf.concat([tf.expand_dims(fused_context, 1), x], axis=-1)\n",
        "\n",
        "#         output, state, _ = self.lstm(x)\n",
        "#         x = self.fc1(output)\n",
        "#         x = tf.reshape(x, (-1, x.shape[2]))\n",
        "#         x = self.fc2(x)\n",
        "#         return x, state\n",
        "\n",
        "#     def reset_state(self, batch_size):\n",
        "#         return tf.zeros((batch_size, self.units))\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# üß† Decoder with Spectral Attention Fusion\n",
        "# ====================================================\n",
        "# class RNN_Decoder(tf.keras.Model):\n",
        "#     def __init__(self, embedding_dim, units, vocab_size):\n",
        "#         super().__init__()\n",
        "#         self.units = units\n",
        "#         self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
        "#         self.lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "#         self.fc1 = layers.Dense(units)\n",
        "#         self.fc2 = layers.Dense(vocab_size)\n",
        "\n",
        "#         self.dual_fusion = DualSpectralFusion(units)\n",
        "\n",
        "#     def call(self, x, yolo_feats, xcep_feats, hidden):\n",
        "#         # Apply dual-branch + spectral attention\n",
        "#         fused_context = self.dual_fusion(yolo_feats, xcep_feats, hidden)  # (B, D)\n",
        "#         x = self.embedding(x)  # (B, 1, D)\n",
        "#         x = tf.concat([tf.expand_dims(fused_context, 1), x], axis=-1)  # (B,1,D+D)\n",
        "\n",
        "#         output, state, _ = self.lstm(x)\n",
        "#         x = self.fc1(output)  # (B,1,H)\n",
        "#         x = tf.reshape(x, (-1, x.shape[2]))  # (B,H)\n",
        "#         x = self.fc2(x)  # (B,vocab)\n",
        "#         return x, state\n",
        "\n",
        "#     def reset_state(self, batch_size):\n",
        "#         return tf.zeros((batch_size, self.units))\n",
        "\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "        self.fc1 = tf.keras.layers.Dense(units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "        self.dual_fusion = DualSpectralFusion(units)\n",
        "\n",
        "    def call(self, x, yolo_feats, xcep_feats, hidden):\n",
        "        fused_context = self.dual_fusion(yolo_feats, xcep_feats, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(fused_context, 1), x], axis=-1)\n",
        "        output, state, _ = self.lstm(x)\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "        return x, state\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "3PuGyx_s-UxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# üß© CNN Encoder (Xception) & YOLO Mock Encoders\n",
        "# =========================\n",
        "# class CNN_Encoder(tf.keras.Model):\n",
        "#     def __init__(self, embedding_dim):\n",
        "#         super(CNN_Encoder, self).__init__()\n",
        "#         self.fc = layers.Dense(embedding_dim)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         x = self.fc(x)\n",
        "#         x = tf.nn.relu(x)\n",
        "#         return x\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.fc = layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.nn.relu(self.fc(x))\n"
      ],
      "metadata": {
        "id": "xvZ2XcDP-XAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class YOLO_Encoder(tf.keras.Model):\n",
        "#     def __init__(self, embedding_dim):\n",
        "#         super(YOLO_Encoder, self).__init__()\n",
        "#         self.fc = layers.Dense(embedding_dim)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         x = self.fc(x)\n",
        "#         x = tf.nn.relu(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class YOLO_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.fc = layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.nn.relu(self.fc(x))\n"
      ],
      "metadata": {
        "id": "39F-cts5-bH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# üöÄ TRAINING PREPARATION\n",
        "# =========================\n",
        "encoder_xcep = CNN_Encoder(MY_EMBEDDING_DIM)\n",
        "encoder_yolo = YOLO_Encoder(MY_EMBEDDING_DIM)\n",
        "decoder = RNN_Decoder(MY_EMBEDDING_DIM, UNIT_COUNT, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "# =========================\n",
        "# üí° Loss Function\n",
        "# =========================\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n"
      ],
      "metadata": {
        "id": "1dDGx2PI-eIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlkPfw6OZi0m"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(os.path.abspath(\"/content/drive/MyDrive/image_captioning/image_captioning\")))\n",
        "file = model_dir + \"/max_length.txt\"  # hadie\n",
        "with open(file, 'r') as filetoread:  # hadie\n",
        "    max_length = int(filetoread.readline())  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAyuJCrbaN8i"
      },
      "outputs": [],
      "source": [
        "# vocab_size = WORD_DICT_SIZE + 1\n",
        "# encoder = CNN_Encoder(MY_EMBEDDING_DIM)\n",
        "# decoder = RNN_Decoder(MY_EMBEDDING_DIM, UNIT_COUNT, vocab_size)\n",
        "# decoder.build(input_shape=(None, max_length)) #diya\n",
        "\n",
        "# decoder.load_weights(model_dir + \"/my_model.weights.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_xcep = CNN_Encoder(MY_EMBEDDING_DIM)\n",
        "# encoder_yolo = YOLO_Encoder(MY_EMBEDDING_DIM)\n",
        "# decoder = RNN_Decoder(MY_EMBEDDING_DIM, UNIT_COUNT, vocab_size)"
      ],
      "metadata": {
        "id": "sPD7LhWJBMxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL_lAUylaRAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fd931a-9b2b-4f24-fbd0-badf06274b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "mod = importlib.import_module(\"feature_extraction_model_\" + feature_extraction_model)  # hadie\n",
        "image_model = mod.image_model  # hadie\n",
        "load_image = mod.load_image  # hadie\n",
        "attention_features_shape = mod.attention_features_shape + 1  # hadie\n",
        "\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ysd92Yrayy0"
      },
      "outputs": [],
      "source": [
        "# loading the tokenizer\n",
        "with open(model_dir + \"/tokenizer.pickle\", 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "features_shape = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFfyUlpha3GF"
      },
      "outputs": [],
      "source": [
        "# def evaluate(image):\n",
        "#     # attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "#     # hidden = decoder.reset_state(batch_size=1)\n",
        "#     # temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "#     # img_tensor_val = image_features_extract_model(temp_input)\n",
        "#     # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "#     # yolo_features = image_path_to_yolo_bounding_boxes(image)  # hadie\n",
        "#     # yolo_features = np.array(yolo_features.flatten())  # hadie\n",
        "#     # yolo_features = np.pad(yolo_features, (0, features_shape - yolo_features.shape[0]), 'constant', constant_values=(0, 0)).astype(np.float32)  # hadie\n",
        "#     # combined_features = np.vstack((img_tensor_val[0].numpy(), yolo_features)).astype(np.float32)  # hadie\n",
        "#     # # features = encoder(combined_features)  # hadie\n",
        "#     # half = img_tensor.shape[0] // 2\n",
        "#     # xcep_feats = img_tensor[:half, :]\n",
        "#     # yolo_feats = img_tensor[half:, :]\n",
        "#     # xcep_encoded = encoder_xcep(xcep_feats)\n",
        "#     # yolo_encoded = encoder_yolo(yolo_feats)\n",
        "#     # predictions, hidden = decoder(dec_input, yolo_encoded, xcep_encoded, hidden)\n",
        "#     # dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "#     result = []\n",
        "\n",
        "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "#     img_tensor_val = image_features_extract_model(temp_input)\n",
        "#     img_tensor_val = tf.reshape(img_tensor_val, (1, -1, img_tensor_val.shape[3]))\n",
        "\n",
        "#     yolo_features = image_path_to_yolo_bounding_boxes(image)\n",
        "#     yolo_features = np.pad(np.array(yolo_features).flatten(), (0, features_shape - len(yolo_features)), 'constant')\n",
        "#     yolo_features = yolo_features.astype(np.float32)[None, :, None] # match dims\n",
        "#     half = img_tensor_val.shape[1] // 2\n",
        "#     xcep_feats = img_tensor_val[:, :half, :]\n",
        "#     yolo_feats = tf.convert_to_tensor(yolo_features)[:, :half, :]\n",
        "\n",
        "#     xcep_encoded = encoder_xcep(xcep_feats)\n",
        "#     yolo_encoded = encoder_yolo(yolo_feats)\n",
        "\n",
        "#     hidden = decoder.reset_state(batch_size=1)\n",
        "#     dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "\n",
        "\n",
        "#     for i in range(max_length):\n",
        "#         predictions, hidden, attention_weights = decoder(dec_input, yolo_encoded, xcep_encoded, hidden)\n",
        "#         attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
        "\n",
        "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "\n",
        "#         # Safe mapping: replace out-of-vocab token IDs with <unk> #diya\n",
        "#         word = tokenizer.index_word.get(predicted_id, '<unk>')\n",
        "#         result.append(word)\n",
        "#         if word == '<end>':\n",
        "#             # return result, attention_plot\n",
        "#             break #diya\n",
        "\n",
        "\n",
        "#         dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "#     attention_plot = attention_plot[:len(result),:]\n",
        "#     return result, attention_plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM4nj7vQcGqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366e155f-8c88-4143-d4ef-efd34ef6755e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "yolo = YOLOv4()\n",
        "\n",
        "# yolo = YOLOv4(tiny=True)\n",
        "\n",
        "yolo.config.parse_names(\"/content/drive/MyDrive/image_captioning/image_captioning/coco.names\")\n",
        "yolo.config.parse_cfg(\"/content/drive/MyDrive/image_captioning/image_captioning/yolov4.cfg\")\n",
        "# yolo.input_size = (480,640)\n",
        "\n",
        "yolo.make_model()\n",
        "yolo.load_weights(\"/content/drive/MyDrive/image_captioning/weights/yolov4.weights\", weights_type=\"yolo\")\n",
        "\n",
        "# yolo.inference(media_path=\"C:/Users/Hadie/Desktop/yolo/NYC_14th_Street_looking_west_12_2005.jpg\")\n",
        "\n",
        "\n",
        "# the output is sorted according to the area by confidence\n",
        "def image_path_to_yolo_bounding_boxes(image_path):  # , coco_dict, word_index):\n",
        "    frame = cv2.imread(image_path)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    bboxes = yolo.predict(frame, prob_thresh=0.25)\n",
        "    bboxes = bboxes.tolist()\n",
        "    n = len(bboxes)\n",
        "    # for each bounding box, append (area * confidence)\n",
        "    for i in range(n):\n",
        "        bboxes[i].append(bboxes[i][2] * bboxes[i][3] * bboxes[i][5])\n",
        "        # obj_class_name = coco_dict[int(bboxes[i][4])].replace(\" \", \"\")\n",
        "        # if obj_class_name in word_index:\n",
        "        #    bboxes[i][4] = word_index[coco_dict[int(bboxes[i][4])].replace(\" \", \"\")]\n",
        "        # else:\n",
        "        #    bboxes[i][4] = word_index['<pad>']\n",
        "    quickSort(bboxes, 0, n - 1)\n",
        "    bboxes = np.array(bboxes)\n",
        "    return bboxes\n",
        "\n",
        "\n",
        "# raw feature extraction - not bounding boxes\n",
        "def yolo_load_image(image_path):\n",
        "    frame = cv2.imread(image_path)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # height, width, _ = frame.shape\n",
        "    frame = yolo.resize_image(frame)\n",
        "    frame = frame / 255.0\n",
        "    frame = frame[np.newaxis, ...].astype(np.float32)\n",
        "    return frame\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw8aohboa-a1"
      },
      "source": [
        "main code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCPCVoDPa9ti"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')  # hadie\n",
        "\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/image_captioning/image_captioning\")\n",
        "sys.path.append(os.getcwd())  # add current folder to Python path\n",
        "\n",
        "os.chdir(os.path.dirname(os.path.abspath(\"/content/drive/MyDrive/image_captioning/image_captioning\")))  # hadie\n",
        "\n",
        "from yolo import image_path_to_yolo_bounding_boxes  # hadie\n",
        "start_date = datetime.datetime.now()  # hadie\n",
        "my_start = timer()  # hadie\n",
        "\n",
        "if not os.path.exists(\"trained_model_\" + feature_extraction_model):  # create the dicrectory if it does not exists # hadie\n",
        "    os.makedirs(\"trained_model_\" + feature_extraction_model)  # hadie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxK69lDKch2i"
      },
      "outputs": [],
      "source": [
        "# Annotation folder path\n",
        "annotation_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/annotations/\"\n",
        "\n",
        "# Pick the right annotation file\n",
        "if split == 0:  # training split\n",
        "    annotation_file = os.path.join(annotation_folder, \"captions_train2014.json\")\n",
        "    image_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/\"\n",
        "\n",
        "else:           # validation split\n",
        "    annotation_file = os.path.join(annotation_folder, \"captions_val2014.json\")\n",
        "    image_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/val2014/\"\n",
        "\n",
        "PATH = image_folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNorwN4odBsB",
        "outputId": "cc55c924-2703-4f09-ab84-a63d3c14577f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training captions:  20000 , all captions:  414113\n"
          ]
        }
      ],
      "source": [
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "all_ids = []  # hadie\n",
        "\n",
        "image_id_index = {}  # hadie\n",
        "for img in annotations['images']:  # hadie\n",
        "    image_id_index[img['id']] = img['file_name']  # hadie\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    if DATASET == \"mscoco\":  # hadie\n",
        "        full_coco_image_path = PATH + image_id_index[image_id]\n",
        "        # print(full_coco_image_path, image_id)\n",
        "    else:  # hadie\n",
        "        full_coco_image_path = PATH + image_id + \".jpg\"  # hadie\n",
        "    all_ids.append(image_id)  # hadie\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "train_ids, train_captions, img_name_vector = shuffle(all_ids,  # hadie\n",
        "                                         all_captions,\n",
        "                                         all_img_name_vector,\n",
        "                                         random_state=1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "num_examples = EXAMPLE_NUMBER  # hadie\n",
        "if LIMIT_SIZE:  # hadie\n",
        "    train_ids = train_ids[:num_examples]\n",
        "    train_captions = train_captions[:num_examples]\n",
        "    img_name_vector = img_name_vector[:num_examples]\n",
        "\n",
        "print(\"training captions: \", len(train_captions), \", all captions: \", len(all_captions))  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00rJ2yPFdEX4"
      },
      "outputs": [],
      "source": [
        "mod = importlib.import_module(\"feature_extraction_model_\" + feature_extraction_model)  # hadie\n",
        "image_model = mod.image_model  # hadie\n",
        "load_image = mod.load_image  # hadie\n",
        "attention_features_shape = mod.attention_features_shape + 1  # hadie\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT62K5QFdJ8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bcd0cde-d155-4218-da81-259396c26eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------START OF EXECUTION-----------------------------\n",
            "extracting features (0) valid file(s)\n",
            "‚úÖ finished extracting features\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------START OF EXECUTION-----------------------------\")\n",
        "\n",
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Only keep unprocessed images\n",
        "encode_train = [\n",
        "    x for x in encode_train\n",
        "    if not os.path.exists(x + \"_\" + feature_extraction_model + \".npy\")\n",
        "]\n",
        "\n",
        "# ‚úÖ Filter out missing files\n",
        "encode_train = [x for x in encode_train if os.path.exists(x)]\n",
        "\n",
        "print(f\"extracting features ({len(encode_train)}) valid file(s)\")  # hadie\n",
        "\n",
        "if len(encode_train) > 0:\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "\n",
        "    # --- Safe loader that skips missing files ---\n",
        "    def safe_load_image(path):\n",
        "        try:\n",
        "            img = tf.io.read_file(path)\n",
        "            img = tf.image.decode_jpeg(img, channels=3)\n",
        "            img = tf.image.resize(img, (299, 299))\n",
        "            img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "            return img, path\n",
        "        except tf.errors.NotFoundError:\n",
        "            # Skip missing or unreadable images\n",
        "            print(f\"‚ö†Ô∏è Skipping missing file: {path.numpy().decode('utf-8')}\")\n",
        "            return None\n",
        "\n",
        "    def filter_none(data):\n",
        "        return data is not None\n",
        "\n",
        "    # Map safely\n",
        "    image_dataset = (\n",
        "        image_dataset\n",
        "        .map(lambda path: tf.py_function(safe_load_image, [path], [tf.float32, tf.string]),\n",
        "             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        .filter(lambda img, path: tf.reduce_all(tf.not_equal(tf.size(img), 0)))  # ensure valid tensors\n",
        "        .batch(16)\n",
        "    )\n",
        "\n",
        "    for img, path in tqdm(image_dataset):\n",
        "        batch_features = image_features_extract_model(img)\n",
        "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "        for bf, p in zip(batch_features, path):\n",
        "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "\n",
        "            # --- YOLO + combined features ---\n",
        "            yolo_features = image_path_to_yolo_bounding_boxes(path_of_feature)\n",
        "            yolo_features = np.array(yolo_features).flatten()\n",
        "            yolo_features = np.pad(yolo_features, (0, features_shape - len(yolo_features)),\n",
        "                                   'constant', constant_values=(0, 0)).astype(np.float32)\n",
        "\n",
        "            combined_features = np.vstack((bf.numpy(), yolo_features)).astype(np.float32)\n",
        "            np.save(path_of_feature + \"_\" + feature_extraction_model, combined_features)\n",
        "\n",
        "print(\"‚úÖ finished extracting features\")  # hadie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I07IeJPKdPCs",
        "outputId": "48ddcf96-b5ec-4057-9dc3-f82e1f9c0ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
            "/tmp/ipython-input-1048036035.py:18: SyntaxWarning: invalid escape sequence '\\]'\n",
            "  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizing and padding captions\n",
            "finished tokenizing and padding captions\n"
          ]
        }
      ],
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = WORD_DICT_SIZE  # hadie\n",
        "\n",
        "if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "    print(\"using the cashed tokenizer\")  # hadie\n",
        "    # loading the tokenizer # hadie\n",
        "    with open(\"trained_model_\" + feature_extraction_model + \"/tokenizer.pickle\", 'rb') as handle:  # hadie\n",
        "        tokenizer = pickle.load(handle)  # hadie\n",
        "else:  # hadie\n",
        "    print(\"tokenizing and padding captions\")  # hadie\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                      oov_token=\"<unk>\",\n",
        "                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "    tokenizer.fit_on_texts(train_captions)\n",
        "    train_seqs = tokenizer.texts_to_sequences(train_captions)  # 777 maybe this line needs removal\n",
        "    tokenizer.word_index['<pad>'] = 0\n",
        "    tokenizer.index_word[0] = '<pad>'\n",
        "    # saving the tokenizer to disk # hadie\n",
        "    with open(\"trained_model_\" + feature_extraction_model + \"/tokenizer.pickle\", 'wb') as handle:  # hadie\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)  # hadie\n",
        "\n",
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "# Pad each vector to the max_length of the captions\n",
        "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
        "\n",
        "if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "    file = \"trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n",
        "    with open(file, 'r') as filetoread:  # hadie\n",
        "        max_length = int(filetoread.readline())  # hadie\n",
        "else:  # hadie\n",
        "    # Calculates the max_length, which is used to store the attention weights\n",
        "    max_length = calc_max_length(train_seqs)\n",
        "\n",
        "    file = \"trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n",
        "    with open(file, 'w') as filetowrite:  # hadie\n",
        "        filetowrite.write(str(max_length))  # write the maximum length to disk # hadie\n",
        "\n",
        "print(\"finished tokenizing and padding captions\")  # hadie\n",
        "\n",
        "\"\"\"## Split the data into training and testing\"\"\"\n",
        "\n",
        "# # Create training and validation sets using an 80-20 split\n",
        "\n",
        "# img_name_train, img_name_val, cap_train, cap_val = train_test_split(\n",
        "#     img_name_vector, cap_vector, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "\n",
        "# new spli w image_id_val\n",
        "image_ids = np.arange(len(img_name_vector))  # assign IDs before splitting\n",
        "\n",
        "image_id_train, image_id_val, img_name_train, img_name_val, cap_train, cap_val = train_test_split(\n",
        "    image_ids, img_name_vector, cap_vector, test_size=0.2, random_state=42\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy0xakGFdVhi",
        "outputId": "2f55a992-37f4-418d-edcf-89ef8cb74648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(img_name_train) =  16000 , len(cap_train) =  16000 , len(img_name_val) =  4000 , len(cap_val) =  4000\n"
          ]
        }
      ],
      "source": [
        "print(\"len(img_name_train) = \", len(img_name_train), \", len(cap_train) = \", len(cap_train), \", len(img_name_val) = \", len(img_name_val), \", len(cap_val) = \", len(cap_val))  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8MJi7andbmF"
      },
      "outputs": [],
      "source": [
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8') + \"_\" + feature_extraction_model + '.npy')\n",
        "  return img_tensor, cap\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2qtl4POddPz"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = MY_OPTIMIZER  # hadie\n",
        "loss_object = MY_LOSS_OBJECT  # hadie\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ8OSEBPdf0a"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "# hadie\n",
        "if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:\n",
        "    try:\n",
        "        for filename in os.listdir(checkpoint_path):\n",
        "            print(\"deleting \" + checkpoint_path + \"/\" + filename)\n",
        "            os.unlink(checkpoint_path + \"/\" + filename)\n",
        "    except Exception as e:\n",
        "        # print('Failed to delete %s. Reason: %s' % (checkpoint_path + \"/\" + filename, e))\n",
        "        print(\"Failed to delete checkpoint(s). Reason:\", e)\n",
        "    # remove the saved model too\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/my_model.index\"):\n",
        "        print(\"deleting trained_model_\" + feature_extraction_model + \"/my_model.index\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/my_model.index\")\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/checkpoint\"):\n",
        "        print(\"deleting /trained_model_\" + feature_extraction_model + \"/checkpoint\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/checkpoint\")\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\"):\n",
        "        print(\"deleting trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\")\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/learning_curve.png\"):\n",
        "        print(\"deleting trained_model_\" + feature_extraction_model + \"/learning_curve.png\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/learning_curve.png\")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) #diya\n",
        "\n",
        "ckpt = tf.train.Checkpoint(encoder_xcep=encoder_xcep,\n",
        "                           encoder_yolo=encoder_yolo,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"‚úÖ Restored dual spectral checkpoint\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB4hrFUUdih1",
        "outputId": "6f48eee0-09c7-4639-d766-0c9f90bd19af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Cleaning dataset ‚Äî skipping missing .npy feature files...\n",
            "‚úÖ Found 11987 valid images.\n",
            "‚ö†Ô∏è Skipped 8013 missing feature files.\n",
            "\n",
            "‚úÖ Dataset built successfully ‚Äî all missing .npy files skipped.\n"
          ]
        }
      ],
      "source": [
        "print(\"üßπ Cleaning dataset ‚Äî skipping missing .npy feature files...\")\n",
        "\n",
        "# Filter all valid feature files before creating dataset\n",
        "valid_img_paths = []\n",
        "missing_count = 0\n",
        "\n",
        "for path in img_name_vector:\n",
        "    feature_path = path + \"_\" + feature_extraction_model + \".npy\"\n",
        "    if os.path.exists(feature_path):\n",
        "        valid_img_paths.append(path)\n",
        "    else:\n",
        "        missing_count += 1\n",
        "\n",
        "print(f\"‚úÖ Found {len(valid_img_paths)} valid images.\")\n",
        "print(f\"‚ö†Ô∏è Skipped {missing_count} missing feature files.\\n\")\n",
        "\n",
        "# Build dataset from only valid files\n",
        "img_name_train = valid_img_paths\n",
        "cap_train = [cap for img, cap in zip(img_name_vector, cap_vector) if img in valid_img_paths]\n",
        "\n",
        "# Now recreate dataset safely\n",
        "def map_func(img_name, cap):\n",
        "    # Decode bytes to string if needed\n",
        "    img_path = img_name.numpy().decode('utf-8')\n",
        "    feature_path = img_path + \"_\" + feature_extraction_model + \".npy\"\n",
        "    # guaranteed to exist from filtering above\n",
        "    img_tensor = np.load(feature_path)\n",
        "    return img_tensor, cap\n",
        "\n",
        "def tf_map_func(img_name, cap):\n",
        "    return tf.py_function(map_func, [img_name, cap], [tf.float32, tf.int32])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "dataset = dataset.shuffle(buffer_size=1000)\n",
        "dataset = dataset.map(tf_map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset = dataset.batch(64)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"‚úÖ Dataset built successfully ‚Äî all missing .npy files skipped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üöÄ Spectral Attention Captioning ‚Äî Training + Evaluation\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os, json, time\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Model Components (already defined above)\n",
        "# ------------------------------------------------------------\n",
        "encoder_xcep = CNN_Encoder(MY_EMBEDDING_DIM)\n",
        "encoder_yolo = YOLO_Encoder(MY_EMBEDDING_DIM)\n",
        "decoder = RNN_Decoder(MY_EMBEDDING_DIM, UNIT_COUNT, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Loss Function\n",
        "# ------------------------------------------------------------\n",
        "@tf.function\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Train Step (Spectral Fusion)\n",
        "# ------------------------------------------------------------\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    \"\"\"\n",
        "    img_tensor: loaded combined npy feature (Xception+YOLO)\n",
        "    target: padded caption tensor\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "    # Split combined features into Xception + YOLO halves\n",
        "    seq_len = img_tensor.shape[1] // 2\n",
        "    xcep_feats = img_tensor[:, :seq_len, :]\n",
        "    yolo_feats = img_tensor[:, seq_len:, :]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        xcep_encoded = encoder_xcep(xcep_feats)\n",
        "        yolo_encoded = encoder_yolo(yolo_feats)\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            preds, hidden = decoder(dec_input, yolo_encoded, xcep_encoded, hidden)\n",
        "            loss += loss_function(target[:, i], preds)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = loss / int(target.shape[1])\n",
        "\n",
        "    vars_all = (\n",
        "        encoder_xcep.trainable_variables +\n",
        "        encoder_yolo.trainable_variables +\n",
        "        decoder.trainable_variables\n",
        "    )\n",
        "    grads = tape.gradient(loss, vars_all)\n",
        "    optimizer.apply_gradients(zip(grads, vars_all))\n",
        "    return total_loss\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ Dataset Loader ‚Äî from your .npy files\n",
        "# ------------------------------------------------------------\n",
        "def map_func(img_name, cap):\n",
        "    feature_path = img_name.numpy().decode('utf-8') + \"_\" + feature_extraction_model + \".npy\"\n",
        "    img_tensor = np.load(feature_path)\n",
        "    img_tensor = tf.convert_to_tensor(img_tensor, dtype=tf.float32)\n",
        "    return img_tensor, cap\n",
        "\n",
        "def tf_map_func(img_name, cap):\n",
        "    return tf.py_function(map_func, [img_name, cap], [tf.float32, tf.int32])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.map(tf_map_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ Checkpointing\n",
        "# ------------------------------------------------------------\n",
        "checkpoint_dir = \"./checkpoints_spectral\"\n",
        "ckpt = tf.train.Checkpoint(\n",
        "    encoder_xcep=encoder_xcep,\n",
        "    encoder_yolo=encoder_yolo,\n",
        "    decoder=decoder,\n",
        "    optimizer=optimizer\n",
        ")\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
        "\n",
        "if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:\n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        for f in os.listdir(checkpoint_dir):\n",
        "            os.remove(os.path.join(checkpoint_dir, f))\n",
        "    print(\"üßπ Cleared old checkpoints\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ Training Loop\n",
        "# ------------------------------------------------------------\n",
        "EPOCHS = 20\n",
        "loss_plot = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    print(f\"\\nüß† Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch, (img_tensor, target) in enumerate(tqdm(dataset)):\n",
        "        batch_loss = train_step(img_tensor, target)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Batch {batch} | Loss: {batch_loss.numpy():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    loss_plot.append(float(avg_loss))\n",
        "    ckpt_manager.save()\n",
        "    print(f\"‚úÖ Epoch {epoch+1} completed | Avg Loss: {avg_loss:.4f} | Time: {time.time()-start:.2f}s\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7Ô∏è‚É£ Save Model + Tokenizer + Training History\n",
        "# ------------------------------------------------------------\n",
        "save_dir = f\"/content/drive/MyDrive/image_captioning/trained_model_spectral\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "encoder_xcep.save_weights(os.path.join(save_dir, \"encoder_xcep.weights.h5\"))\n",
        "encoder_yolo.save_weights(os.path.join(save_dir, \"encoder_yolo.weights.h5\"))\n",
        "decoder.save_weights(os.path.join(save_dir, \"decoder.weights.h5\"))\n",
        "\n",
        "with open(os.path.join(save_dir, \"loss_plot.json\"), \"w\") as f:\n",
        "    json.dump(loss_plot, f)\n",
        "\n",
        "with open(os.path.join(save_dir, \"tokenizer.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"‚úÖ Training complete. Models saved to:\", save_dir)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8Ô∏è‚É£ Evaluation (Generate captions)\n",
        "# ------------------------------------------------------------\n",
        "def evaluate_image(image_path):\n",
        "    \"\"\"Generate a caption for one image using spectral attention decoder.\"\"\"\n",
        "    img_tensor = np.load(image_path + \"_\" + feature_extraction_model + \".npy\")\n",
        "    img_tensor = tf.expand_dims(img_tensor, 0)\n",
        "\n",
        "    seq_len = img_tensor.shape[1] // 2\n",
        "    xcep_feats = img_tensor[:, :seq_len, :]\n",
        "    yolo_feats = img_tensor[:, seq_len:, :]\n",
        "\n",
        "    xcep_encoded = encoder_xcep(xcep_feats)\n",
        "    yolo_encoded = encoder_yolo(yolo_feats)\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    result = []\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden = decoder(dec_input, yolo_encoded, xcep_encoded, hidden)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        word = tokenizer.index_word.get(predicted_id, '')\n",
        "        if word == '<end>':\n",
        "            break\n",
        "        result.append(word)\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 9Ô∏è‚É£ Run Evaluation on Validation Subset\n",
        "# ------------------------------------------------------------\n",
        "val_results = []\n",
        "sample_imgs = img_name_val[:500]  # evaluate on 500 images\n",
        "\n",
        "for img_path in tqdm(sample_imgs):\n",
        "    if not os.path.exists(img_path + \"_\" + feature_extraction_model + \".npy\"):\n",
        "        continue\n",
        "    caption = evaluate_image(img_path)\n",
        "    val_results.append({\n",
        "        \"image_id\": os.path.basename(img_path),\n",
        "        \"predicted_caption\": caption\n",
        "    })\n",
        "\n",
        "results_path = os.path.join(save_dir, \"results_val.json\")\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(val_results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Saved {len(val_results)} validation captions ‚Üí {results_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RSQDd8Tdob9",
        "outputId": "e4acb990-429c-448d-83ea-59f90bfc2403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Cleared old checkpoints\n",
            "\n",
            "üß† Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [01:07<7:02:19, 67.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 2.2507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [28:58<1:19:52, 17.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 1.1902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [54:57<45:13, 15.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 1.1587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [1:20:26<20:10, 16.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.9692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [1:39:15<00:00, 15.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1 completed | Avg Loss: 1.1583 | Time: 5956.33s\n",
            "\n",
            "üß† Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<05:46,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.8960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [00:53<02:18,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.9318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [01:44<01:31,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.9198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:36<00:39,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.8947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:12<00:00,  1.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2 completed | Avg Loss: 0.9120 | Time: 193.89s\n",
            "\n",
            "üß† Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:18,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.7609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [00:52<02:13,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.8490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [01:43<01:27,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.7628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:33<00:36,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.7320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:10<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 3 completed | Avg Loss: 0.8232 | Time: 191.65s\n",
            "\n",
            "üß† Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:24,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.7951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [00:54<02:26,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.7201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [01:47<01:28,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.8137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:39<00:36,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.7749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:21<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 4 completed | Avg Loss: 0.7594 | Time: 202.86s\n",
            "\n",
            "üß† Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:50,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.7471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [00:57<02:19,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.6274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [01:48<01:27,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.7493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:40<00:36,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.6957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:17<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 5 completed | Avg Loss: 0.7082 | Time: 198.40s\n",
            "\n",
            "üß† Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<05:13,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.7868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [00:56<02:16,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.6402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [01:50<01:39,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.6882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:42<00:36,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.7113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:19<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 6 completed | Avg Loss: 0.6646 | Time: 201.21s\n",
            "\n",
            "üß† Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<03:58,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.6737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [00:59<03:27,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.6711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [01:52<01:27,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.6478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:43<00:37,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.6024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:20<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 7 completed | Avg Loss: 0.6243 | Time: 202.40s\n",
            "\n",
            "üß† Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:15,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.5103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:09<02:16,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.6615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:14<01:25,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.6056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:05<00:36,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.5757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:42<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 8 completed | Avg Loss: 0.5872 | Time: 223.66s\n",
            "\n",
            "üß† Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<03:45,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.5909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:00<02:14,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.6486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [01:51<01:24,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.5685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:43<00:37,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.5170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:21<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 9 completed | Avg Loss: 0.5524 | Time: 202.33s\n",
            "\n",
            "üß† Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:05,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.4845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:15<04:37,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.4703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:14<01:34,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.4942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:05<00:38,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.4786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:42<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 10 completed | Avg Loss: 0.5181 | Time: 223.96s\n",
            "\n",
            "üß† Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:25,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.5171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:19<02:41,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.4998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:35<01:28,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.4738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:26<00:37,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.5003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [04:04<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 11 completed | Avg Loss: 0.4861 | Time: 246.18s\n",
            "\n",
            "üß† Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<03:58,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.4191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:24<06:13,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.4705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:33<01:33,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.4730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:32<00:38,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.4960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [04:10<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 12 completed | Avg Loss: 0.4548 | Time: 251.76s\n",
            "\n",
            "üß† Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:05,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.3937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:31<06:27,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.4555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:47<01:27,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.3943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:40<00:37,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.4349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [04:18<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 13 completed | Avg Loss: 0.4239 | Time: 259.75s\n",
            "\n",
            "üß† Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:03,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.4363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:31<03:54,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.4188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:53<01:34,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.3735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:45<00:39,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.3684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [04:25<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 14 completed | Avg Loss: 0.3949 | Time: 266.71s\n",
            "\n",
            "üß† Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:18,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.4235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:43<07:36,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.3285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [03:19<01:58,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.3405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [04:44<00:38,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.3738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [05:30<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 15 completed | Avg Loss: 0.3662 | Time: 331.76s\n",
            "\n",
            "üß† Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:21,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.3686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:15<02:23,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.3743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:11<01:27,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.3809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:04<00:37,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.3287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:47<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 16 completed | Avg Loss: 0.3394 | Time: 228.70s\n",
            "\n",
            "üß† Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<03:58,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.3246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:39<04:24,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.3017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [03:17<03:07,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.3644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [04:33<00:40,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.3489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [05:14<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 17 completed | Avg Loss: 0.3132 | Time: 315.31s\n",
            "\n",
            "üß† Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:18,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.3185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:08<02:34,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.2837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:06<01:26,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.2828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [02:58<00:37,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.3070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [03:44<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 18 completed | Avg Loss: 0.2882 | Time: 226.00s\n",
            "\n",
            "üß† Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<04:34,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.2745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:31<05:07,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.2472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [02:37<01:29,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.2787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [03:30<00:37,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.2659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [04:09<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 19 completed | Avg Loss: 0.2641 | Time: 250.86s\n",
            "\n",
            "üß† Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/375 [00:00<03:52,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 | Loss: 0.2401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|‚ñà‚ñà‚ñã       | 101/375 [01:37<02:36,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 | Loss: 0.2377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 201/375 [03:23<03:57,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200 | Loss: 0.2562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 301/375 [04:54<01:17,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 300 | Loss: 0.2545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [06:04<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 20 completed | Avg Loss: 0.2442 | Time: 365.57s\n",
            "‚úÖ Training complete. Models saved to: /content/drive/MyDrive/image_captioning/trained_model_spectral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [03:44<00:00,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 311 validation captions ‚Üí /content/drive/MyDrive/image_captioning/trained_model_spectral/results_val.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opUKbkgmYuAR"
      },
      "outputs": [],
      "source": [
        "# loss_plot = []\n",
        "\n",
        "# @tf.function\n",
        "# def train_step(img_tensor, target):\n",
        "#     loss = 0\n",
        "#     hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "#     dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "#     # Split the .npy file into YOLO + Xception sub-features\n",
        "#     half = img_tensor.shape[1] // 2\n",
        "#     xcep_feats = img_tensor[:, :half, :]\n",
        "#     yolo_feats = img_tensor[:, half:, :]\n",
        "\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         xcep_encoded = encoder_xcep(xcep_feats)\n",
        "#         yolo_encoded = encoder_yolo(yolo_feats)\n",
        "#         for i in range(1, target.shape[1]):\n",
        "#             predictions, hidden = decoder(dec_input, yolo_encoded, xcep_encoded, hidden)\n",
        "#             loss += loss_function(target[:, i], predictions)\n",
        "#             dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "#     total_loss = loss / int(target.shape[1])\n",
        "#     variables = (encoder_yolo.trainable_variables +\n",
        "#                  encoder_xcep.trainable_variables +\n",
        "#                  decoder.trainable_variables)\n",
        "#     gradients = tape.gradient(loss, variables)\n",
        "#     optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "#     return loss, total_loss\n",
        "\n",
        "\n",
        "# EPOCHS = 20  # hadie\n",
        "\n",
        "# # üß† Helper to safely load batches\n",
        "# def safe_batch_loader(dataset):\n",
        "#     \"\"\"Yield batches, skipping missing files gracefully.\"\"\"\n",
        "#     for batch, (img_tensor, target) in enumerate(dataset):\n",
        "#         try:\n",
        "#             yield batch, (img_tensor, target)\n",
        "#         except (FileNotFoundError, tf.errors.NotFoundError) as e:\n",
        "#             print(f\"‚ö†Ô∏è Skipping missing or corrupted file in batch {batch}: {e}\")\n",
        "#             continue\n",
        "\n",
        "\n",
        "# if not os.path.exists(\"trained_model_\" + feature_extraction_model + \"/my_model.index\"):  # hadie\n",
        "#     print(\"training..\")  # hadie\n",
        "#     for epoch in range(start_epoch, EPOCHS):\n",
        "#         print(f\"Starting epoch {epoch+1}/{EPOCHS}\")\n",
        "#         start = time.time()\n",
        "#         total_loss = 0\n",
        "\n",
        "#         for batch, (img_tensor, target) in safe_batch_loader(dataset):\n",
        "#             try:\n",
        "#                 batch_loss, t_loss = train_step(img_tensor, target)\n",
        "#                 total_loss += t_loss\n",
        "#             except (FileNotFoundError, tf.errors.NotFoundError) as e:\n",
        "#                 print(f\"‚ö†Ô∏è Skipping batch {batch} due to missing file: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#             if batch % 100 == 0:\n",
        "#                 print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n",
        "#                 print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "#                     epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "\n",
        "#         if len(dataset) > 0:\n",
        "#             avg_loss = total_loss / len(dataset)\n",
        "#         else:\n",
        "#             avg_loss = 0\n",
        "\n",
        "#         print(f\"Epoch {epoch+1} Loss {avg_loss:.4f}\")\n",
        "#         loss_plot.append(avg_loss)\n",
        "\n",
        "#         if epoch % 5 == 0:\n",
        "#             ckpt_manager.save()\n",
        "\n",
        "#         print(f\"Epoch {epoch+1} Loss {avg_loss:.6f}\")\n",
        "#         print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\\n\")\n",
        "\n",
        "#     # Save model\n",
        "#     save_dir = f\"trained_model_{feature_extraction_model}\"\n",
        "\n",
        "#     base_dir = \"/content/drive/MyDrive/image_captioning\"\n",
        "\n",
        "#     save_dir = os.path.join(base_dir, f\"trained_model_{feature_extraction_model}\")\n",
        "\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "#     decoder.save_weights(os.path.join(save_dir, \"my_model.weights.h5\"))\n",
        "\n",
        "# else:  # hadie\n",
        "#     print(\"A trained model has been found. Loading it from disk..\")  # hadie\n",
        "#     decoder.load_weights(\"trained_model_\" + feature_extraction_model + \"/my_model.weights.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDy92FC9rcG-"
      },
      "outputs": [],
      "source": [
        "# encoder_xcep.save_weights(os.path.join(save_dir, \"encoder_xcep.weights.h5\"))\n",
        "# encoder_yolo.save_weights(os.path.join(save_dir, \"encoder_yolo.weights.h5\"))\n",
        "# decoder.save_weights(os.path.join(save_dir, \"decoder.weights.h5\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOm2-ebPrfmr"
      },
      "outputs": [],
      "source": [
        "# with open(os.path.join(save_dir, \"loss_plot.json\"), \"w\") as f:\n",
        "#     json.dump([float(l) for l in loss_plot], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNS_I_Terl98"
      },
      "outputs": [],
      "source": [
        "# with open(os.path.join(save_dir, \"tokenizer.pkl\"), \"wb\") as f:\n",
        "#     pickle.dump(tokenizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENXAorsVroOx"
      },
      "outputs": [],
      "source": [
        "# #load loss and tokeniser\n",
        "# save_dir = f\"trained_model_{feature_extraction_model}\"\n",
        "\n",
        "# with open(os.path.join(save_dir, \"loss_plot.json\")) as f:\n",
        "#     loss_plot = json.load(f)\n",
        "\n",
        "# with open(os.path.join(save_dir, \"tokenizer.pkl\"), \"rb\") as f:\n",
        "#     tokenizer = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mktoGHZidmUA"
      },
      "outputs": [],
      "source": [
        "# def plot_attention(image, result, attention_plot):\n",
        "#     temp_image = np.array(Image.open(image))\n",
        "\n",
        "#     fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "#     len_result = len(result)\n",
        "#     for l in range(len_result):\n",
        "#         temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "#         ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n",
        "#         ax.set_title(result[l])\n",
        "#         img = ax.imshow(temp_image)\n",
        "#         ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oakV9LlHdoCL"
      },
      "outputs": [],
      "source": [
        "# print(\"Total img_name_val:\", len(img_name_val))\n",
        "# print(\"Example path:\", img_name_val[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQFXTOlUdpuP"
      },
      "outputs": [],
      "source": [
        "# def validate_image(id, image_name_for_validation, original_caption):\n",
        "#   try:\n",
        "#     print(\"Evaluating:\", image_name_for_validation.decode(\"utf-8\"), \"From thread\", threading.current_thread().ident)\n",
        "\n",
        "#     result, attention_plot = evaluate(image_name_for_validation)  # generate the hypothesis\n",
        "#     result = ' '.join(result).replace(\"<end>\", \"\").strip()  # remove unnecessary characters\n",
        "\n",
        "#     dict = {}\n",
        "#     dict[\"image_id\"] = int(id)  # .decode(\"utf-8\")\n",
        "#     dict[\"caption\"] = result;\n",
        "#     dict[\"original_caption\"] = original_caption.decode(\"utf-8\")\n",
        "#     dict[\"file_name\"] = image_name_for_validation.decode(\"utf-8\")\n",
        "#     return json.dumps(dict)\n",
        "#   except Exception as e:\n",
        "#         print(\"‚ö†Ô∏è Skipping image:\", image_name_for_validation.decode(\"utf-8\"), \"Error:\", e)\n",
        "#         return json.dumps({})  # return an empty dict so the pipeline doesn't break\n",
        "\n",
        "# caption_strings_val = list(map(lambda item: (' '.join([tokenizer.index_word[i] for i in item if i not in [0]])).replace(\"<end>\", \"\").replace(\"<start>\", \"\").strip(), cap_val))  # convert to a list of strings\n",
        "# validation_dataset = tf.data.Dataset.from_tensor_slices((image_id_val, img_name_val, caption_strings_val))  # create the dataset\n",
        "# validation_dataset = validation_dataset.take(300)  # only evaluate first 100 images\n",
        "# eval_start_date = datetime.datetime.now()  # hadie\n",
        "# list_of_dicts = validation_dataset.map(lambda item1, item2, item3: tf.numpy_function(validate_image, [item1, item2, item3], [tf.string]), num_parallel_calls=1)  # run in parallel\n",
        "# list_of_dicts = list(list_of_dicts.as_numpy_iterator())  # convert to a list\n",
        "# list_of_dicts = [item for sublist in list_of_dicts for item in sublist]  # flatten the list by removing nested tuples\n",
        "# list_of_dicts = list(map(lambda item: json.loads(item), list_of_dicts))  # rewrap the strings as dictionaries, then convert to a list\n",
        "\n",
        "# added_ids = []\n",
        "\n",
        "# # remove empty dicts (those from skipped images)\n",
        "# list_of_dicts = [d for d in list_of_dicts if \"image_id\" in d]\n",
        "\n",
        "# unique_list_of_dicts = []\n",
        "# for d in list_of_dicts:\n",
        "#     if d[\"image_id\"] not in added_ids:\n",
        "#         added_ids.append(d[\"image_id\"])\n",
        "#         unique_list_of_dicts.append(d)\n",
        "\n",
        "# file = \"trained_model_\" + feature_extraction_model + \"/results.json\"  # hadie\n",
        "# with open(file, 'w') as filetowrite:  # hadie\n",
        "#     filetowrite.write(json.dumps(unique_list_of_dicts))  # hadie\n",
        "\n",
        "# print(\"The results have been written to trained_model_\" + feature_extraction_model + \"/results.json\")  # hadie\n",
        "# print(\"Main thread:\", threading.current_thread().ident)  # hadie\n",
        "\n",
        "# end_date = datetime.datetime.now()  # hadie\n",
        "\n",
        "# my_end = timer()  # hadie\n",
        "# hours, rem = divmod(my_end - my_start, 3600)  # hadie\n",
        "# minutes, seconds = divmod(rem, 60)  # hadie\n",
        "\n",
        "# print(\"Start time: \" + str(start_date))  # hadie\n",
        "# print(\"Evaluation start time: \" + str(eval_start_date))  # hadie\n",
        "# print(\"End time: \" + str(end_date))  # hadie\n",
        "\n",
        "# print(\"Time elapsed (hours:minutes:seconds): {:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds))  # hadie\n",
        "\n",
        "# if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "#     plt.plot(loss_plot)\n",
        "#     plt.xlabel('Epochs')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.title('Loss Plot')\n",
        "#     plt.savefig(\"trained_model_\" + feature_extraction_model + \"/learning_curve.png\")  # hadie\n",
        "#     # plt.show() # hadie\n",
        "\n",
        "# print(\"The learning curve has been written to trained_model_\" + feature_extraction_model + \"/learning_curve.png\")  # hadie\n",
        "# print(\"Feature extraction model: \" + feature_extraction_model)\n",
        "# print(\"Dataset: \" + DATASET)\n",
        "# print(\"Development set proportion: \" + str(TEST_SET_PROPORTION))\n",
        "# print(\"with yolo bounding boxes\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/val2014\"  # or test2014 if you have it\n",
        "\n",
        "\n",
        "# img_name_test = sorted([\n",
        "#     os.path.join(test_folder, f)\n",
        "#     for f in os.listdir(test_folder)\n",
        "#     if f.lower().endswith(\".jpg\")\n",
        "# ])\n",
        "\n",
        "# # ‚úÖ 3. Filter only those with extracted .npy features\n",
        "# img_name_test = [\n",
        "#     path for path in img_name_test\n",
        "#     if os.path.exists(path + \"_\" + feature_extraction_model + \".npy\")\n",
        "# ]"
      ],
      "metadata": {
        "id": "ZndobpCmXVWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json, os\n",
        "# from tqdm import tqdm\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "\n",
        "# results = []\n",
        "\n",
        "# # Directory where results.json should be saved\n",
        "# results_path = os.path.join(save_dir, \"results.json\")\n",
        "\n",
        "# # ‚úÖ Ensure you have a loaded model (encoder + decoder)\n",
        "# encoder_xcep.load_weights(os.path.join(save_dir, \"encoder_xcep.weights.h5\"))\n",
        "# encoder_yolo.load_weights(os.path.join(save_dir, \"encoder_yolo.weights.h5\"))\n",
        "# decoder.load_weights(os.path.join(save_dir, \"decoder.weights.h5\"))\n",
        "\n",
        "\n",
        "# def evaluate_image(image_path):\n",
        "#     \"\"\"Generate a caption for one image.\"\"\"\n",
        "#     # Load precomputed features\n",
        "#     img_tensor = np.load(image_path + \"_\" + feature_extraction_model + \".npy\")\n",
        "#     img_tensor = tf.convert_to_tensor(img_tensor)\n",
        "#     img_tensor = tf.expand_dims(img_tensor, 0)\n",
        "\n",
        "#     half = img_tensor.shape[1] // 2\n",
        "#     xcep_feats = img_tensor[:, :half, :]\n",
        "#     yolo_feats = img_tensor[:, half:, :]\n",
        "\n",
        "#     xcep_encoded = encoder_xcep(xcep_feats)\n",
        "#     yolo_encoded = encoder_yolo(yolo_feats)\n",
        "\n",
        "#     result = []\n",
        "\n",
        "#     for i in range(40):  # max caption length\n",
        "#         predictions, hidden = decoder(dec_input, yolo_encoded, xcep_encoded, hidden)\n",
        "\n",
        "\n",
        "#         predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "#         word = tokenizer.index_word.get(predicted_id, '')\n",
        "#         if word == '<end>':\n",
        "#             break\n",
        "#         result.append(word)\n",
        "#         dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "#     return ' '.join(result)\n",
        "\n",
        "# # ‚úÖ Evaluate a subset or all test images\n",
        "# test_images = img_name_test[:50]  # or full test set\n",
        "# for img_path in tqdm(test_images):\n",
        "#     if not os.path.exists(img_path + \"_\" + feature_extraction_model + \".npy\"):\n",
        "#         continue\n",
        "#     predicted_caption = evaluate_image(img_path)\n",
        "#     results.append({\n",
        "#         \"image_id\": os.path.basename(img_path),\n",
        "#         \"predicted_caption\": predicted_caption\n",
        "#     })\n",
        "\n",
        "# # ‚úÖ Save results to Drive\n",
        "# with open(results_path, \"w\") as f:\n",
        "#     json.dump(results, f, indent=2)\n",
        "\n",
        "# print(f\"‚úÖ Saved {len(results)} predictions to {results_path}\")\n"
      ],
      "metadata": {
        "id": "qASZr5LZXJ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load COCO annotations\n",
        "# with open('/content/drive/MyDrive/image_captioning/datasets/coco2014/annotations/captions_val2014.json') as f:\n",
        "#     val_data = json.load(f)\n",
        "\n",
        "# results = []\n",
        "\n",
        "# # Evaluate on a small subset first (say 500 images for speed)\n",
        "# for ann in tqdm(val_data['annotations'][:500]):\n",
        "#     img_id = ann['image_id']\n",
        "#     img_path = f\"/content/drive/MyDrive/image_captioning/datasets/coco2014/val2014/COCO_val2014_{img_id:012d}.jpg\"\n",
        "\n",
        "#     # Get predicted caption\n",
        "#     pred_caption, _ = evaluate(img_path)\n",
        "#     pred_caption = ' '.join(pred_caption).replace('<start>', '').replace('<end>', '').strip()\n",
        "\n",
        "#     results.append({\n",
        "#         \"image_id\": img_id,\n",
        "#         \"caption\": pred_caption\n",
        "#     })\n",
        "\n",
        "# # Save predictions in COCO format\n",
        "# with open(\"results.json\", \"w\") as f:\n",
        "#     json.dump(results, f)\n"
      ],
      "metadata": {
        "id": "oObfWDgpMapq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(results[:5])"
      ],
      "metadata": {
        "id": "BlEn0Uw7SR1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}