{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doooooodlesssss/imagecap/blob/main/imgcap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lfr1lKzhZHZ",
        "outputId": "ca63d0b7-ef79-4470-a720-a80080b27c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFQfmRu4iaXy",
        "outputId": "4ce38b0d-8487-41b1-e10b-71b490cc13b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<1.27 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.12/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: yolov4 in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "‚úÖ Patched YOLOv4 weights.py\n",
            "Call tf.config.experimental.set_memory_growth(GPU0, True)\n"
          ]
        }
      ],
      "source": [
        "# ‚ö†Ô∏è Restart runtime first, then run this cleanly:\n",
        "!pip install \"numpy<1.27\" opencv-python==4.6.0.66 yolov4\n",
        "\n",
        "# Patch the YOLOv4 weights.py\n",
        "file_path = \"/usr/local/lib/python3.12/dist-packages/yolov4/tf/utils/weights.py\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "code = code.replace(\n",
        "    \"conv_shape = (filters, conv2d.input_shape[-1], *conv2d.kernel_size)\",\n",
        "    \"conv_shape = (filters, conv2d.input.shape[-1], *conv2d.kernel_size)\"\n",
        ")\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"‚úÖ Patched YOLOv4 weights.py\")\n",
        "\n",
        "# Import YOLOv4 directly after patch\n",
        "from yolov4.tf import YOLOv4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQx3qAtZKM6F",
        "outputId": "bda19c96-13ec-47ec-c766-790a4cec01c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/image_captioning/image_captioning\n"
          ]
        }
      ],
      "source": [
        "repo_path = \"/content/image_captioning\"\n",
        "%cd /content/drive/MyDrive/image_captioning/image_captioning/\n",
        "\n",
        "kaggle_json = \"/content/drive/MyDrive/image_captioning/kaggle.json\"\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/image_captioning/datasets/coco2014\"\n",
        "\n",
        "feature_extraction_model = \"xception\"\n",
        "\n",
        "model_dir = \"trained_model_xception\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFLyFBzcKPzn",
        "outputId": "7f44c594-eda1-4d4b-d1c9-89cdcca443a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow (preinstalled): 2.19.0\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "‚úîÔ∏è opencv-python-headless==4.6.0.66 already installed\n",
            "‚úîÔ∏è matplotlib already installed\n",
            "‚úîÔ∏è pillow already installed\n",
            "‚úîÔ∏è tqdm already installed\n",
            "‚úîÔ∏è pycocotools already installed\n",
            "‚úîÔ∏è lxml already installed\n",
            "‚úîÔ∏è pandas already installed\n",
            "‚úîÔ∏è seaborn already installed\n",
            "‚úîÔ∏è yolov4 already installed\n",
            "tensorflow already installed ‚úÖ\n",
            "kaggle already installed ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow (preinstalled):\", tf.__version__)\n",
        "import os, importlib.util\n",
        "import sys, pkgutil\n",
        "print(\"Python:\", sys.version)\n",
        "\n",
        "def safe_pip(pkg, import_name=None):\n",
        "    if import_name is None:\n",
        "        import_name = pkg.split(\"==\")[0]\n",
        "    if importlib.util.find_spec(import_name) is None:\n",
        "        !pip install {pkg}\n",
        "    else:\n",
        "        print(f\"‚úîÔ∏è {pkg} already installed\")\n",
        "\n",
        "safe_pip(\"opencv-python-headless==4.6.0.66\", \"cv2\")\n",
        "safe_pip(\"matplotlib\")\n",
        "safe_pip(\"pillow\", \"PIL\")\n",
        "safe_pip(\"tqdm\")\n",
        "safe_pip(\"pycocotools\")\n",
        "safe_pip(\"lxml\")\n",
        "safe_pip(\"pandas\")\n",
        "safe_pip(\"seaborn\")\n",
        "safe_pip(\"yolov4\")\n",
        "\n",
        "if importlib.util.find_spec(\"tensorflow\") is None:\n",
        "    !pip install tensorflow\n",
        "else:\n",
        "    print(\"tensorflow already installed ‚úÖ\")\n",
        "\n",
        "if importlib.util.find_spec(\"kaggle\") is None:\n",
        "    !pip install kaggle\n",
        "else:\n",
        "    print(\"kaggle already installed ‚úÖ\")\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import datetime\n",
        "import importlib\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "from builtins import len\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "import threading\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "P0ZeK1zqeUvF"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "WORD_DICT_SIZE = 15000\n",
        "LIMIT_SIZE = True\n",
        "EXAMPLE_NUMBER = 20000  # will only work if LIMIT_SIZE is True\n",
        "MY_EMBEDDING_DIM = 256\n",
        "UNIT_COUNT = 512\n",
        "MY_OPTIMIZER = tf.keras.optimizers.Adam()\n",
        "MY_LOSS_OBJECT = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "EPOCH_COUNT = 20\n",
        "REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN = True\n",
        "DATASET = \"mscoco\"  # \"mscoco\" or \"flickr8k\" or \"flickr30k\"\n",
        "TEST_SET_PROPORTION = 1\n",
        "feature_extraction_model = \"xception\"\n",
        "split = 0  # 0 for training, 1 for testing\n",
        "\n",
        "BATCH_SIZE = 32  # 64\n",
        "BUFFER_SIZE = 1000  # 1000\n",
        "embedding_dim = MY_EMBEDDING_DIM  # hadie\n",
        "units = UNIT_COUNT  # hadie\n",
        "top_k = WORD_DICT_SIZE\n",
        "vocab_size = top_k + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t27s38Q-czw4"
      },
      "outputs": [],
      "source": [
        "# Python program for implementation of Quicksort Sort\n",
        "def partition(arr, low, high):\n",
        "\ti = (low - 1)\n",
        "\tpivot = arr[high]\n",
        "\tfor j in range(low, high):\n",
        "\t\tif arr[j][6] >= pivot[6]:\n",
        "\t\t\ti = i + 1\n",
        "\t\t\tarr[i], arr[j] = arr[j], arr[i]\n",
        "\tarr[i + 1], arr[high] = arr[high], arr[i + 1]\n",
        "\treturn (i + 1)\n",
        "def quickSort(arr, low, high):\n",
        "\tif len(arr) == 1:\n",
        "\t\treturn arr\n",
        "\tif low < high:\n",
        "\t\tpi = partition(arr, low, high)\n",
        "\t\tquickSort(arr, low, pi - 1)\n",
        "\t\tquickSort(arr, pi + 1, high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Z_SlEbT2K5Ay"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"/content/drive/MyDrive/image_captioning/image_captioning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BzB-qRlCYtGU"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # you get 1 at the last axis because you are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the LSTM\n",
        "    output, state, _ = self.lstm(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LlkPfw6OZi0m"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(os.path.abspath(\"/content/drive/MyDrive/image_captioning/image_captioning\")))\n",
        "file = model_dir + \"/max_length.txt\"  # hadie\n",
        "with open(file, 'r') as filetoread:  # hadie\n",
        "    max_length = int(filetoread.readline())  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VAyuJCrbaN8i"
      },
      "outputs": [],
      "source": [
        "vocab_size = WORD_DICT_SIZE + 1\n",
        "encoder = CNN_Encoder(MY_EMBEDDING_DIM)\n",
        "decoder = RNN_Decoder(MY_EMBEDDING_DIM, UNIT_COUNT, vocab_size)\n",
        "decoder.build(input_shape=(None, max_length)) #diya\n",
        "\n",
        "decoder.load_weights(model_dir + \"/my_model.weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AL_lAUylaRAX"
      },
      "outputs": [],
      "source": [
        "mod = importlib.import_module(\"feature_extraction_model_\" + feature_extraction_model)  # hadie\n",
        "image_model = mod.image_model  # hadie\n",
        "load_image = mod.load_image  # hadie\n",
        "attention_features_shape = mod.attention_features_shape + 1  # hadie\n",
        "\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_Ysd92Yrayy0"
      },
      "outputs": [],
      "source": [
        "# loading the tokenizer\n",
        "with open(model_dir + \"/tokenizer.pickle\", 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "features_shape = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CFfyUlpha3GF"
      },
      "outputs": [],
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    yolo_features = image_path_to_yolo_bounding_boxes(image)  # hadie\n",
        "    yolo_features = np.array(yolo_features.flatten())  # hadie\n",
        "    yolo_features = np.pad(yolo_features, (0, features_shape - yolo_features.shape[0]), 'constant', constant_values=(0, 0)).astype(np.float32)  # hadie\n",
        "    combined_features = np.vstack((img_tensor_val[0].numpy(), yolo_features)).astype(np.float32)  # hadie\n",
        "    features = encoder(combined_features)  # hadie\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    result = []\n",
        "\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "\n",
        "        # result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        # Safe mapping: replace out-of-vocab token IDs with <unk> #diya\n",
        "        word = tokenizer.index_word.get(predicted_id, '<unk>')\n",
        "        result.append(word)\n",
        "        if word == '<end>':\n",
        "            # return result, attention_plot\n",
        "            break #diya\n",
        "\n",
        "\n",
        "\n",
        "        # if tokenizer.index_word[predicted_id] == '<end>':\n",
        "        #     return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result),:]\n",
        "    return result, attention_plot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# result = []\n",
        "\n",
        "# for i in range(max_caption_length):\n",
        "#     predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "#     predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "\n",
        "#     # üîí Safe: replace unknown IDs with <unk>\n",
        "#     word = tokenizer.index_word.get(predicted_id, '<unk>')\n",
        "#     result.append(word)\n",
        "\n",
        "#     # stop if <end> predicted\n",
        "#     if word == '<end>':\n",
        "#         break\n",
        "\n",
        "#     # prepare next input\n",
        "#     dec_input = tf.expand_dims([predicted_id], 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "t2GheqXNa5sv"
      },
      "outputs": [],
      "source": [
        "# # image = sys.argv[1]\n",
        "# image = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/val2014/COCO_val2014_000000000073.jpg\"  # path to your image\n",
        "\n",
        "\n",
        "# result, attention_plot = evaluate(image)\n",
        "# file_name = ('caption_sample_' + str(datetime.datetime.now()) + \".png\").replace(\":\", \"_\").replace(\" \", \"__\")\n",
        "# print ('Prediction Caption:`', ' '.join(result))\n",
        "# # plt.savefig(file_name)\n",
        "# print(\"The results have been written to \" + file_name)\n",
        "# # plot_attention(image, result, attention_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iM4nj7vQcGqs"
      },
      "outputs": [],
      "source": [
        "yolo = YOLOv4()\n",
        "\n",
        "# yolo = YOLOv4(tiny=True)\n",
        "\n",
        "yolo.config.parse_names(\"/content/drive/MyDrive/image_captioning/image_captioning/coco.names\")\n",
        "yolo.config.parse_cfg(\"/content/drive/MyDrive/image_captioning/image_captioning/yolov4.cfg\")\n",
        "# yolo.input_size = (480,640)\n",
        "\n",
        "yolo.make_model()\n",
        "yolo.load_weights(\"/content/drive/MyDrive/image_captioning/weights/yolov4.weights\", weights_type=\"yolo\")\n",
        "\n",
        "# yolo.inference(media_path=\"C:/Users/Hadie/Desktop/yolo/NYC_14th_Street_looking_west_12_2005.jpg\")\n",
        "\n",
        "\n",
        "# the output is sorted according to the area by confidence\n",
        "def image_path_to_yolo_bounding_boxes(image_path):  # , coco_dict, word_index):\n",
        "    frame = cv2.imread(image_path)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    bboxes = yolo.predict(frame, prob_thresh=0.25)\n",
        "    bboxes = bboxes.tolist()\n",
        "    n = len(bboxes)\n",
        "    # for each bounding box, append (area * confidence)\n",
        "    for i in range(n):\n",
        "        bboxes[i].append(bboxes[i][2] * bboxes[i][3] * bboxes[i][5])\n",
        "        # obj_class_name = coco_dict[int(bboxes[i][4])].replace(\" \", \"\")\n",
        "        # if obj_class_name in word_index:\n",
        "        #    bboxes[i][4] = word_index[coco_dict[int(bboxes[i][4])].replace(\" \", \"\")]\n",
        "        # else:\n",
        "        #    bboxes[i][4] = word_index['<pad>']\n",
        "    quickSort(bboxes, 0, n - 1)\n",
        "    bboxes = np.array(bboxes)\n",
        "    return bboxes\n",
        "\n",
        "\n",
        "# raw feature extraction - not bounding boxes\n",
        "def yolo_load_image(image_path):\n",
        "    frame = cv2.imread(image_path)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # height, width, _ = frame.shape\n",
        "    frame = yolo.resize_image(frame)\n",
        "    frame = frame / 255.0\n",
        "    frame = frame[np.newaxis, ...].astype(np.float32)\n",
        "    return frame\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw8aohboa-a1"
      },
      "source": [
        "main code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NCPCVoDPa9ti"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')  # hadie\n",
        "\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/image_captioning/image_captioning\")\n",
        "sys.path.append(os.getcwd())  # add current folder to Python path\n",
        "\n",
        "os.chdir(os.path.dirname(os.path.abspath(\"/content/drive/MyDrive/image_captioning/image_captioning\")))  # hadie\n",
        "\n",
        "from yolo import image_path_to_yolo_bounding_boxes  # hadie\n",
        "start_date = datetime.datetime.now()  # hadie\n",
        "my_start = timer()  # hadie\n",
        "\n",
        "if not os.path.exists(\"trained_model_\" + feature_extraction_model):  # create the dicrectory if it does not exists # hadie\n",
        "    os.makedirs(\"trained_model_\" + feature_extraction_model)  # hadie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DxK69lDKch2i"
      },
      "outputs": [],
      "source": [
        "# Annotation folder path\n",
        "annotation_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/annotations/\"\n",
        "\n",
        "# Pick the right annotation file\n",
        "if split == 0:  # training split\n",
        "    annotation_file = os.path.join(annotation_folder, \"captions_train2014.json\")\n",
        "    image_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/\"\n",
        "\n",
        "else:           # validation split\n",
        "    annotation_file = os.path.join(annotation_folder, \"captions_val2014.json\")\n",
        "    image_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/val2014/\"\n",
        "\n",
        "PATH = image_folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNorwN4odBsB",
        "outputId": "e88c6cc8-ba1d-45cd-8468-ae25239bbbfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training captions:  20000 , all captions:  414113\n"
          ]
        }
      ],
      "source": [
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "all_ids = []  # hadie\n",
        "\n",
        "image_id_index = {}  # hadie\n",
        "for img in annotations['images']:  # hadie\n",
        "    image_id_index[img['id']] = img['file_name']  # hadie\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    if DATASET == \"mscoco\":  # hadie\n",
        "        full_coco_image_path = PATH + image_id_index[image_id]\n",
        "        # print(full_coco_image_path, image_id)\n",
        "    else:  # hadie\n",
        "        full_coco_image_path = PATH + image_id + \".jpg\"  # hadie\n",
        "    all_ids.append(image_id)  # hadie\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "train_ids, train_captions, img_name_vector = shuffle(all_ids,  # hadie\n",
        "                                         all_captions,\n",
        "                                         all_img_name_vector,\n",
        "                                         random_state=1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "num_examples = EXAMPLE_NUMBER  # hadie\n",
        "if LIMIT_SIZE:  # hadie\n",
        "    train_ids = train_ids[:num_examples]\n",
        "    train_captions = train_captions[:num_examples]\n",
        "    img_name_vector = img_name_vector[:num_examples]\n",
        "\n",
        "print(\"training captions: \", len(train_captions), \", all captions: \", len(all_captions))  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "00rJ2yPFdEX4"
      },
      "outputs": [],
      "source": [
        "mod = importlib.import_module(\"feature_extraction_model_\" + feature_extraction_model)  # hadie\n",
        "image_model = mod.image_model  # hadie\n",
        "load_image = mod.load_image  # hadie\n",
        "attention_features_shape = mod.attention_features_shape + 1  # hadie\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cT62K5QFdJ8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10699e85-c9b8-4d69-d288-fe2d50978f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------START OF EXECUTION-----------------------------\n",
            "extracting features (0) valid file(s)\n",
            "‚úÖ finished extracting features\n"
          ]
        }
      ],
      "source": [
        "print(\"-----------------------------START OF EXECUTION-----------------------------\")\n",
        "\n",
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Only keep unprocessed images\n",
        "encode_train = [\n",
        "    x for x in encode_train\n",
        "    if not os.path.exists(x + \"_\" + feature_extraction_model + \".npy\")\n",
        "]\n",
        "\n",
        "# ‚úÖ Filter out missing files\n",
        "encode_train = [x for x in encode_train if os.path.exists(x)]\n",
        "\n",
        "print(f\"extracting features ({len(encode_train)}) valid file(s)\")  # hadie\n",
        "\n",
        "if len(encode_train) > 0:\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "\n",
        "    # --- Safe loader that skips missing files ---\n",
        "    def safe_load_image(path):\n",
        "        try:\n",
        "            img = tf.io.read_file(path)\n",
        "            img = tf.image.decode_jpeg(img, channels=3)\n",
        "            img = tf.image.resize(img, (299, 299))\n",
        "            img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "            return img, path\n",
        "        except tf.errors.NotFoundError:\n",
        "            # Skip missing or unreadable images\n",
        "            print(f\"‚ö†Ô∏è Skipping missing file: {path.numpy().decode('utf-8')}\")\n",
        "            return None\n",
        "\n",
        "    def filter_none(data):\n",
        "        return data is not None\n",
        "\n",
        "    # Map safely\n",
        "    image_dataset = (\n",
        "        image_dataset\n",
        "        .map(lambda path: tf.py_function(safe_load_image, [path], [tf.float32, tf.string]),\n",
        "             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        .filter(lambda img, path: tf.reduce_all(tf.not_equal(tf.size(img), 0)))  # ensure valid tensors\n",
        "        .batch(16)\n",
        "    )\n",
        "\n",
        "    for img, path in tqdm(image_dataset):\n",
        "        batch_features = image_features_extract_model(img)\n",
        "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "        for bf, p in zip(batch_features, path):\n",
        "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "\n",
        "            # --- YOLO + combined features ---\n",
        "            yolo_features = image_path_to_yolo_bounding_boxes(path_of_feature)\n",
        "            yolo_features = np.array(yolo_features).flatten()\n",
        "            yolo_features = np.pad(yolo_features, (0, features_shape - len(yolo_features)),\n",
        "                                   'constant', constant_values=(0, 0)).astype(np.float32)\n",
        "\n",
        "            combined_features = np.vstack((bf.numpy(), yolo_features)).astype(np.float32)\n",
        "            np.save(path_of_feature + \"_\" + feature_extraction_model, combined_features)\n",
        "\n",
        "print(\"‚úÖ finished extracting features\")  # hadie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I07IeJPKdPCs",
        "outputId": "3ffb582d-0c54-4188-8ecb-4f7472fff2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizing and padding captions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\]'\n",
            "/tmp/ipython-input-1048036035.py:18: SyntaxWarning: invalid escape sequence '\\]'\n",
            "  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished tokenizing and padding captions\n"
          ]
        }
      ],
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = WORD_DICT_SIZE  # hadie\n",
        "\n",
        "if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "    print(\"using the cashed tokenizer\")  # hadie\n",
        "    # loading the tokenizer # hadie\n",
        "    with open(\"trained_model_\" + feature_extraction_model + \"/tokenizer.pickle\", 'rb') as handle:  # hadie\n",
        "        tokenizer = pickle.load(handle)  # hadie\n",
        "else:  # hadie\n",
        "    print(\"tokenizing and padding captions\")  # hadie\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                      oov_token=\"<unk>\",\n",
        "                                                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "    tokenizer.fit_on_texts(train_captions)\n",
        "    train_seqs = tokenizer.texts_to_sequences(train_captions)  # 777 maybe this line needs removal\n",
        "    tokenizer.word_index['<pad>'] = 0\n",
        "    tokenizer.index_word[0] = '<pad>'\n",
        "    # saving the tokenizer to disk # hadie\n",
        "    with open(\"trained_model_\" + feature_extraction_model + \"/tokenizer.pickle\", 'wb') as handle:  # hadie\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)  # hadie\n",
        "\n",
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "# Pad each vector to the max_length of the captions\n",
        "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
        "\n",
        "if not REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "    file = \"trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n",
        "    with open(file, 'r') as filetoread:  # hadie\n",
        "        max_length = int(filetoread.readline())  # hadie\n",
        "else:  # hadie\n",
        "    # Calculates the max_length, which is used to store the attention weights\n",
        "    max_length = calc_max_length(train_seqs)\n",
        "\n",
        "    file = \"trained_model_\" + feature_extraction_model + \"/max_length.txt\"  # hadie\n",
        "    with open(file, 'w') as filetowrite:  # hadie\n",
        "        filetowrite.write(str(max_length))  # write the maximum length to disk # hadie\n",
        "\n",
        "print(\"finished tokenizing and padding captions\")  # hadie\n",
        "\n",
        "\"\"\"## Split the data into training and testing\"\"\"\n",
        "\n",
        "# # Create training and validation sets using an 80-20 split\n",
        "\n",
        "# img_name_train, img_name_val, cap_train, cap_val = train_test_split(\n",
        "#     img_name_vector, cap_vector, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "\n",
        "# new spli w image_id_val\n",
        "image_ids = np.arange(len(img_name_vector))  # assign IDs before splitting\n",
        "\n",
        "image_id_train, image_id_val, img_name_train, img_name_val, cap_train, cap_val = train_test_split(\n",
        "    image_ids, img_name_vector, cap_vector, test_size=0.2, random_state=42\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy0xakGFdVhi",
        "outputId": "a31a2089-6cc7-42aa-daac-038ea09e62fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(img_name_train) =  16000 , len(cap_train) =  16000 , len(img_name_val) =  4000 , len(cap_val) =  4000\n"
          ]
        }
      ],
      "source": [
        "print(\"len(img_name_train) = \", len(img_name_train), \", len(cap_train) = \", len(cap_train), \", len(img_name_val) = \", len(img_name_val), \", len(cap_val) = \", len(cap_val))  # hadie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-8MJi7andbmF"
      },
      "outputs": [],
      "source": [
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8') + \"_\" + feature_extraction_model + '.npy')\n",
        "  return img_tensor, cap\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "o2qtl4POddPz"
      },
      "outputs": [],
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "optimizer = MY_OPTIMIZER  # hadie\n",
        "loss_object = MY_LOSS_OBJECT  # hadie\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qZ8OSEBPdf0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f2d341-453c-43ba-9d32-e8082b06d670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deleting ./checkpoints/train/ckpt-1.data-00000-of-00001\n",
            "deleting ./checkpoints/train/ckpt-1.index\n",
            "deleting ./checkpoints/train/checkpoint\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "# hadie\n",
        "if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:\n",
        "    try:\n",
        "        for filename in os.listdir(checkpoint_path):\n",
        "            print(\"deleting \" + checkpoint_path + \"/\" + filename)\n",
        "            os.unlink(checkpoint_path + \"/\" + filename)\n",
        "    except Exception as e:\n",
        "        # print('Failed to delete %s. Reason: %s' % (checkpoint_path + \"/\" + filename, e))\n",
        "        print(\"Failed to delete checkpoint(s). Reason:\", e)\n",
        "    # remove the saved model too\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/my_model.index\"):\n",
        "        print(\"deleting trained_model_\" + feature_extraction_model + \"/my_model.index\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/my_model.index\")\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/checkpoint\"):\n",
        "        print(\"deleting /trained_model_\" + feature_extraction_model + \"/checkpoint\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/checkpoint\")\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\"):\n",
        "        print(\"deleting trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/my_model.data-00000-of-00001\")\n",
        "    if os.path.exists(\"./trained_model_\" + feature_extraction_model + \"/learning_curve.png\"):\n",
        "        print(\"deleting trained_model_\" + feature_extraction_model + \"/learning_curve.png\")\n",
        "        os.unlink(\"./trained_model_\" + feature_extraction_model + \"/learning_curve.png\")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) #diya\n",
        "\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB4hrFUUdih1",
        "outputId": "3875dfc6-3095-48ab-a967-78592cc215a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Cleaning dataset ‚Äî skipping missing .npy feature files...\n",
            "‚úÖ Found 11987 valid images.\n",
            "‚ö†Ô∏è Skipped 8013 missing feature files.\n",
            "\n",
            "‚úÖ Dataset built successfully ‚Äî all missing .npy files skipped.\n"
          ]
        }
      ],
      "source": [
        "print(\"üßπ Cleaning dataset ‚Äî skipping missing .npy feature files...\")\n",
        "\n",
        "# Filter all valid feature files before creating dataset\n",
        "valid_img_paths = []\n",
        "missing_count = 0\n",
        "\n",
        "for path in img_name_vector:\n",
        "    feature_path = path + \"_\" + feature_extraction_model + \".npy\"\n",
        "    if os.path.exists(feature_path):\n",
        "        valid_img_paths.append(path)\n",
        "    else:\n",
        "        missing_count += 1\n",
        "\n",
        "print(f\"‚úÖ Found {len(valid_img_paths)} valid images.\")\n",
        "print(f\"‚ö†Ô∏è Skipped {missing_count} missing feature files.\\n\")\n",
        "\n",
        "# Build dataset from only valid files\n",
        "img_name_train = valid_img_paths\n",
        "cap_train = [cap for img, cap in zip(img_name_vector, cap_vector) if img in valid_img_paths]\n",
        "\n",
        "# Now recreate dataset safely\n",
        "def map_func(img_name, cap):\n",
        "    # Decode bytes to string if needed\n",
        "    img_path = img_name.numpy().decode('utf-8')\n",
        "    feature_path = img_path + \"_\" + feature_extraction_model + \".npy\"\n",
        "    # guaranteed to exist from filtering above\n",
        "    img_tensor = np.load(feature_path)\n",
        "    return img_tensor, cap\n",
        "\n",
        "def tf_map_func(img_name, cap):\n",
        "    return tf.py_function(map_func, [img_name, cap], [tf.float32, tf.int32])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "dataset = dataset.shuffle(buffer_size=1000)\n",
        "dataset = dataset.map(tf_map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset = dataset.batch(64)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"‚úÖ Dataset built successfully ‚Äî all missing .npy files skipped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"üì∏ Total test images found: {len(img_name_test)}\")"
      ],
      "metadata": {
        "id": "xDZq2UkvYdVz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opUKbkgmYuAR",
        "outputId": "c9f829d0-b212-49dc-e8d9-d4743422b87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training..\n",
            "Starting epoch 1/20\n",
            "Epoch 1 Batch 0 Loss 113.1457\n",
            "Epoch 1 Batch 0 Loss 2.3091\n",
            "Epoch 1 Batch 100 Loss 56.2297\n",
            "Epoch 1 Batch 100 Loss 1.1475\n",
            "Epoch 1 Loss 1.2239\n",
            "Epoch 1 Loss 1.223855\n",
            "Time taken for 1 epoch 3139.80 sec\n",
            "\n",
            "Starting epoch 2/20\n",
            "Epoch 2 Batch 0 Loss 51.4264\n",
            "Epoch 2 Batch 0 Loss 1.0495\n",
            "Epoch 2 Batch 100 Loss 45.1979\n",
            "Epoch 2 Batch 100 Loss 0.9224\n",
            "Epoch 2 Loss 0.9717\n",
            "Epoch 2 Loss 0.971686\n",
            "Time taken for 1 epoch 150.91 sec\n",
            "\n",
            "Starting epoch 3/20\n",
            "Epoch 3 Batch 0 Loss 42.7482\n",
            "Epoch 3 Batch 0 Loss 0.8724\n",
            "Epoch 3 Batch 100 Loss 42.1084\n",
            "Epoch 3 Batch 100 Loss 0.8594\n",
            "Epoch 3 Loss 0.8476\n",
            "Epoch 3 Loss 0.847588\n",
            "Time taken for 1 epoch 149.75 sec\n",
            "\n",
            "Starting epoch 4/20\n",
            "Epoch 4 Batch 0 Loss 41.9912\n",
            "Epoch 4 Batch 0 Loss 0.8570\n",
            "Epoch 4 Batch 100 Loss 38.5713\n",
            "Epoch 4 Batch 100 Loss 0.7872\n",
            "Epoch 4 Loss 0.7729\n",
            "Epoch 4 Loss 0.772911\n",
            "Time taken for 1 epoch 150.70 sec\n",
            "\n",
            "Starting epoch 5/20\n",
            "Epoch 5 Batch 0 Loss 32.9892\n",
            "Epoch 5 Batch 0 Loss 0.6732\n",
            "Epoch 5 Batch 100 Loss 35.6564\n",
            "Epoch 5 Batch 100 Loss 0.7277\n",
            "Epoch 5 Loss 0.7217\n",
            "Epoch 5 Loss 0.721675\n",
            "Time taken for 1 epoch 148.42 sec\n",
            "\n",
            "Starting epoch 6/20\n",
            "Epoch 6 Batch 0 Loss 31.8237\n",
            "Epoch 6 Batch 0 Loss 0.6495\n",
            "Epoch 6 Batch 100 Loss 34.3749\n",
            "Epoch 6 Batch 100 Loss 0.7015\n",
            "Epoch 6 Loss 0.6770\n",
            "Epoch 6 Loss 0.676973\n",
            "Time taken for 1 epoch 147.74 sec\n",
            "\n",
            "Starting epoch 7/20\n",
            "Epoch 7 Batch 0 Loss 32.8232\n",
            "Epoch 7 Batch 0 Loss 0.6699\n",
            "Epoch 7 Batch 100 Loss 32.4735\n",
            "Epoch 7 Batch 100 Loss 0.6627\n",
            "Epoch 7 Loss 0.6359\n",
            "Epoch 7 Loss 0.635926\n",
            "Time taken for 1 epoch 149.25 sec\n",
            "\n",
            "Starting epoch 8/20\n",
            "Epoch 8 Batch 0 Loss 29.9023\n",
            "Epoch 8 Batch 0 Loss 0.6103\n",
            "Epoch 8 Batch 100 Loss 28.8506\n",
            "Epoch 8 Batch 100 Loss 0.5888\n",
            "Epoch 8 Loss 0.5963\n",
            "Epoch 8 Loss 0.596258\n",
            "Time taken for 1 epoch 148.06 sec\n",
            "\n",
            "Starting epoch 9/20\n",
            "Epoch 9 Batch 0 Loss 28.7551\n",
            "Epoch 9 Batch 0 Loss 0.5868\n",
            "Epoch 9 Batch 100 Loss 25.5245\n",
            "Epoch 9 Batch 100 Loss 0.5209\n",
            "Epoch 9 Loss 0.5576\n",
            "Epoch 9 Loss 0.557618\n",
            "Time taken for 1 epoch 146.35 sec\n",
            "\n",
            "Starting epoch 10/20\n",
            "Epoch 10 Batch 0 Loss 25.0843\n",
            "Epoch 10 Batch 0 Loss 0.5119\n",
            "Epoch 10 Batch 100 Loss 23.5512\n",
            "Epoch 10 Batch 100 Loss 0.4806\n",
            "Epoch 10 Loss 0.5184\n",
            "Epoch 10 Loss 0.518389\n",
            "Time taken for 1 epoch 147.60 sec\n",
            "\n",
            "Starting epoch 11/20\n",
            "Epoch 11 Batch 0 Loss 23.4659\n",
            "Epoch 11 Batch 0 Loss 0.4789\n",
            "Epoch 11 Batch 100 Loss 24.5965\n",
            "Epoch 11 Batch 100 Loss 0.5020\n",
            "Epoch 11 Loss 0.4799\n",
            "Epoch 11 Loss 0.479871\n",
            "Time taken for 1 epoch 148.86 sec\n",
            "\n",
            "Starting epoch 12/20\n",
            "Epoch 12 Batch 0 Loss 22.5854\n",
            "Epoch 12 Batch 0 Loss 0.4609\n",
            "Epoch 12 Batch 100 Loss 21.4634\n",
            "Epoch 12 Batch 100 Loss 0.4380\n",
            "Epoch 12 Loss 0.4427\n",
            "Epoch 12 Loss 0.442711\n",
            "Time taken for 1 epoch 147.77 sec\n",
            "\n",
            "Starting epoch 13/20\n",
            "Epoch 13 Batch 0 Loss 20.5446\n",
            "Epoch 13 Batch 0 Loss 0.4193\n",
            "Epoch 13 Batch 100 Loss 22.1696\n",
            "Epoch 13 Batch 100 Loss 0.4524\n",
            "Epoch 13 Loss 0.4060\n",
            "Epoch 13 Loss 0.406025\n",
            "Time taken for 1 epoch 147.81 sec\n",
            "\n",
            "Starting epoch 14/20\n",
            "Epoch 14 Batch 0 Loss 20.2289\n",
            "Epoch 14 Batch 0 Loss 0.4128\n",
            "Epoch 14 Batch 100 Loss 19.0303\n",
            "Epoch 14 Batch 100 Loss 0.3884\n",
            "Epoch 14 Loss 0.3692\n",
            "Epoch 14 Loss 0.369202\n",
            "Time taken for 1 epoch 147.21 sec\n",
            "\n",
            "Starting epoch 15/20\n",
            "Epoch 15 Batch 0 Loss 17.8613\n",
            "Epoch 15 Batch 0 Loss 0.3645\n",
            "Epoch 15 Batch 100 Loss 17.1443\n",
            "Epoch 15 Batch 100 Loss 0.3499\n",
            "Epoch 15 Loss 0.3350\n",
            "Epoch 15 Loss 0.335040\n",
            "Time taken for 1 epoch 147.55 sec\n",
            "\n",
            "Starting epoch 16/20\n",
            "Epoch 16 Batch 0 Loss 15.6155\n",
            "Epoch 16 Batch 0 Loss 0.3187\n",
            "Epoch 16 Batch 100 Loss 14.9345\n",
            "Epoch 16 Batch 100 Loss 0.3048\n",
            "Epoch 16 Loss 0.3039\n",
            "Epoch 16 Loss 0.303942\n",
            "Time taken for 1 epoch 148.36 sec\n",
            "\n",
            "Starting epoch 17/20\n",
            "Epoch 17 Batch 0 Loss 13.5179\n",
            "Epoch 17 Batch 0 Loss 0.2759\n",
            "Epoch 17 Batch 100 Loss 13.3682\n",
            "Epoch 17 Batch 100 Loss 0.2728\n",
            "Epoch 17 Loss 0.2723\n",
            "Epoch 17 Loss 0.272312\n",
            "Time taken for 1 epoch 201.96 sec\n",
            "\n",
            "Starting epoch 18/20\n",
            "Epoch 18 Batch 0 Loss 15.3728\n",
            "Epoch 18 Batch 0 Loss 0.3137\n",
            "Epoch 18 Batch 100 Loss 10.7907\n",
            "Epoch 18 Batch 100 Loss 0.2202\n",
            "Epoch 18 Loss 0.2450\n",
            "Epoch 18 Loss 0.244951\n",
            "Time taken for 1 epoch 148.90 sec\n",
            "\n",
            "Starting epoch 19/20\n",
            "Epoch 19 Batch 0 Loss 13.7212\n",
            "Epoch 19 Batch 0 Loss 0.2800\n",
            "Epoch 19 Batch 100 Loss 12.7713\n",
            "Epoch 19 Batch 100 Loss 0.2606\n",
            "Epoch 19 Loss 0.2216\n",
            "Epoch 19 Loss 0.221553\n",
            "Time taken for 1 epoch 148.91 sec\n",
            "\n",
            "Starting epoch 20/20\n",
            "Epoch 20 Batch 0 Loss 10.4011\n",
            "Epoch 20 Batch 0 Loss 0.2123\n",
            "Epoch 20 Batch 100 Loss 9.3936\n",
            "Epoch 20 Batch 100 Loss 0.1917\n",
            "Epoch 20 Loss 0.1959\n",
            "Epoch 20 Loss 0.195922\n",
            "Time taken for 1 epoch 149.20 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss_plot = []\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = loss / int(target.shape[1])\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss\n",
        "\n",
        "\n",
        "EPOCHS = 20  # hadie\n",
        "\n",
        "# üß† Helper to safely load batches\n",
        "def safe_batch_loader(dataset):\n",
        "    \"\"\"Yield batches, skipping missing files gracefully.\"\"\"\n",
        "    for batch, (img_tensor, target) in enumerate(dataset):\n",
        "        try:\n",
        "            yield batch, (img_tensor, target)\n",
        "        except (FileNotFoundError, tf.errors.NotFoundError) as e:\n",
        "            print(f\"‚ö†Ô∏è Skipping missing or corrupted file in batch {batch}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "if not os.path.exists(\"trained_model_\" + feature_extraction_model + \"/my_model.index\"):  # hadie\n",
        "    print(\"training..\")  # hadie\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        print(f\"Starting epoch {epoch+1}/{EPOCHS}\")\n",
        "        start = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch, (img_tensor, target) in safe_batch_loader(dataset):\n",
        "            try:\n",
        "                batch_loss, t_loss = train_step(img_tensor, target)\n",
        "                total_loss += t_loss\n",
        "            except (FileNotFoundError, tf.errors.NotFoundError) as e:\n",
        "                print(f\"‚ö†Ô∏è Skipping batch {batch} due to missing file: {e}\")\n",
        "                continue\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "                    epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "\n",
        "        if len(dataset) > 0:\n",
        "            avg_loss = total_loss / len(dataset)\n",
        "        else:\n",
        "            avg_loss = 0\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Loss {avg_loss:.4f}\")\n",
        "        loss_plot.append(avg_loss)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            ckpt_manager.save()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Loss {avg_loss:.6f}\")\n",
        "        print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\\n\")\n",
        "\n",
        "    # Save model\n",
        "    save_dir = f\"trained_model_{feature_extraction_model}\"\n",
        "\n",
        "    base_dir = \"/content/drive/MyDrive/image_captioning\"\n",
        "\n",
        "    save_dir = os.path.join(base_dir, f\"trained_model_{feature_extraction_model}\")\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    decoder.save_weights(os.path.join(save_dir, \"my_model.weights.h5\"))\n",
        "\n",
        "else:  # hadie\n",
        "    print(\"A trained model has been found. Loading it from disk..\")  # hadie\n",
        "    decoder.load_weights(\"trained_model_\" + feature_extraction_model + \"/my_model.weights.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "IDy92FC9rcG-"
      },
      "outputs": [],
      "source": [
        "encoder.save_weights(os.path.join(save_dir, \"encoder.weights.h5\"))\n",
        "decoder.save_weights(os.path.join(save_dir, \"decoder.weights.h5\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "oOm2-ebPrfmr"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(save_dir, \"loss_plot.json\"), \"w\") as f:\n",
        "    json.dump([float(l) for l in loss_plot], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tNS_I_Terl98"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(save_dir, \"tokenizer.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ENXAorsVroOx"
      },
      "outputs": [],
      "source": [
        "#load loss and tokeniser\n",
        "save_dir = f\"trained_model_{feature_extraction_model}\"\n",
        "\n",
        "with open(os.path.join(save_dir, \"loss_plot.json\")) as f:\n",
        "    loss_plot = json.load(f)\n",
        "\n",
        "with open(os.path.join(save_dir, \"tokenizer.pkl\"), \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mktoGHZidmUA"
      },
      "outputs": [],
      "source": [
        "# def evaluate(image):\n",
        "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "#     hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "#     img_tensor_val = image_features_extract_model(temp_input)\n",
        "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "#     # yolo_features = image_path_to_yolo_bounding_boxes(image.decode(\"utf-8\"))  # hadie\n",
        "\n",
        "#     yolo_features = image_path_to_yolo_bounding_boxes(image) #diya\n",
        "\n",
        "\n",
        "#     yolo_features = np.array(yolo_features.flatten())  # hadie\n",
        "#     yolo_features = np.pad(yolo_features, (0, features_shape - yolo_features.shape[0]), 'constant', constant_values=(0, 0)).astype(np.float32)  # hadie\n",
        "#     combined_features = np.vstack((img_tensor_val[0].numpy(), yolo_features)).astype(np.float32)  # hadie\n",
        "#     features = encoder(combined_features)  # hadie\n",
        "\n",
        "#     dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "#     result = []\n",
        "\n",
        "#     for i in range(max_length):\n",
        "#         predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "#         attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
        "\n",
        "#         predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "#         result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "          # # Safe mapping: replace out-of-vocab token IDs with <unk> #diyaaa\n",
        "          # word = tokenizer.index_word.get(predicted_id, '<unk>')\n",
        "          # result.append(word)\n",
        "\n",
        "          # if word == '<end>':\n",
        "          #     return result, attention_plot\n",
        "\n",
        "\n",
        "#         if tokenizer.index_word[predicted_id] == '<end>':\n",
        "#             return result, attention_plot\n",
        "\n",
        "#         dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "#     attention_plot = attention_plot[:len(result),:]\n",
        "#     return result, attention_plot\n",
        "\n",
        "\n",
        "\n",
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result // 2, len_result // 2, l + 1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "oakV9LlHdoCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa07a1e-a94f-4d9c-b71f-bb4b731eaaa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total img_name_val: 4000\n",
            "Example path: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000124931.jpg\n"
          ]
        }
      ],
      "source": [
        "print(\"Total img_name_val:\", len(img_name_val))\n",
        "print(\"Example path:\", img_name_val[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "dQFXTOlUdpuP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d72655a-0fde-432d-e912-04375937cc48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000124931.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000124931.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000389698.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000389698.jpg Error: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000389698.jpg; No such file or directory [Op:ReadFile]\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000002007.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000002007.jpg Error: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000002007.jpg; No such file or directory [Op:ReadFile]\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000003234.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000003234.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000357069.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000357069.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000156296.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000156296.jpg Error: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000156296.jpg; No such file or directory [Op:ReadFile]\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000092221.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000092221.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000303387.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000303387.jpg Error: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000303387.jpg; No such file or directory [Op:ReadFile]\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000181782.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000181782.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000101989.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000101989.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000277810.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000277810.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000157041.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000157041.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000545670.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000545670.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000183031.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000183031.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000162937.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000162937.jpg Error: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000162937.jpg; No such file or directory [Op:ReadFile]\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000009735.jpg From thread 139673728964160\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000009735.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000073189.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000073189.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000369823.jpg From thread 139674021525056\n",
            "‚ö†Ô∏è Skipping image: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000369823.jpg Error: Can't convert object to 'str' for 'filename'\n",
            "Evaluating: /content/drive/MyDrive/image_captioning/datasets/coco2014/train2014/COCO_train2014_000000435433.jpg From thread 139674021525056\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1641493579.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0meval_start_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# hadie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mlist_of_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mlist_of_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# convert to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mlist_of_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_dicts\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# flatten the list by removing nested tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mlist_of_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# rewrap the strings as dictionaries, then convert to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4786\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3082\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m         \"output_shapes\", output_shapes)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def validate_image(id, image_name_for_validation, original_caption):\n",
        "  try:\n",
        "    print(\"Evaluating:\", image_name_for_validation.decode(\"utf-8\"), \"From thread\", threading.current_thread().ident)\n",
        "\n",
        "    result, attention_plot = evaluate(image_name_for_validation)  # generate the hypothesis\n",
        "    result = ' '.join(result).replace(\"<end>\", \"\").strip()  # remove unnecessary characters\n",
        "\n",
        "    dict = {}\n",
        "    dict[\"image_id\"] = int(id)  # .decode(\"utf-8\")\n",
        "    dict[\"caption\"] = result;\n",
        "    dict[\"original_caption\"] = original_caption.decode(\"utf-8\")\n",
        "    dict[\"file_name\"] = image_name_for_validation.decode(\"utf-8\")\n",
        "    return json.dumps(dict)\n",
        "  except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Skipping image:\", image_name_for_validation.decode(\"utf-8\"), \"Error:\", e)\n",
        "        return json.dumps({})  # return an empty dict so the pipeline doesn't break\n",
        "\n",
        "caption_strings_val = list(map(lambda item: (' '.join([tokenizer.index_word[i] for i in item if i not in [0]])).replace(\"<end>\", \"\").replace(\"<start>\", \"\").strip(), cap_val))  # convert to a list of strings\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((image_id_val, img_name_val, caption_strings_val))  # create the dataset\n",
        "validation_dataset = validation_dataset.take(300)  # only evaluate first 100 images\n",
        "eval_start_date = datetime.datetime.now()  # hadie\n",
        "list_of_dicts = validation_dataset.map(lambda item1, item2, item3: tf.numpy_function(validate_image, [item1, item2, item3], [tf.string]), num_parallel_calls=1)  # run in parallel\n",
        "list_of_dicts = list(list_of_dicts.as_numpy_iterator())  # convert to a list\n",
        "list_of_dicts = [item for sublist in list_of_dicts for item in sublist]  # flatten the list by removing nested tuples\n",
        "list_of_dicts = list(map(lambda item: json.loads(item), list_of_dicts))  # rewrap the strings as dictionaries, then convert to a list\n",
        "\n",
        "added_ids = []\n",
        "\n",
        "# remove empty dicts (those from skipped images)\n",
        "list_of_dicts = [d for d in list_of_dicts if \"image_id\" in d]\n",
        "\n",
        "unique_list_of_dicts = []\n",
        "for d in list_of_dicts:\n",
        "    if d[\"image_id\"] not in added_ids:\n",
        "        added_ids.append(d[\"image_id\"])\n",
        "        unique_list_of_dicts.append(d)\n",
        "\n",
        "\n",
        "unique_list_of_dicts = []\n",
        "for dict in list_of_dicts:\n",
        "    if not dict[\"image_id\"] in added_ids:\n",
        "        added_ids.append(dict[\"image_id\"])\n",
        "        unique_list_of_dicts.append(dict)\n",
        "\n",
        "file = \"trained_model_\" + feature_extraction_model + \"/results.json\"  # hadie\n",
        "with open(file, 'w') as filetowrite:  # hadie\n",
        "    filetowrite.write(json.dumps(unique_list_of_dicts))  # hadie\n",
        "\n",
        "print(\"The results have been written to trained_model_\" + feature_extraction_model + \"/results.json\")  # hadie\n",
        "print(\"Main thread:\", threading.current_thread().ident)  # hadie\n",
        "\n",
        "end_date = datetime.datetime.now()  # hadie\n",
        "\n",
        "my_end = timer()  # hadie\n",
        "hours, rem = divmod(my_end - my_start, 3600)  # hadie\n",
        "minutes, seconds = divmod(rem, 60)  # hadie\n",
        "\n",
        "print(\"Start time: \" + str(start_date))  # hadie\n",
        "print(\"Evaluation start time: \" + str(eval_start_date))  # hadie\n",
        "print(\"End time: \" + str(end_date))  # hadie\n",
        "\n",
        "print(\"Time elapsed (hours:minutes:seconds): {:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds))  # hadie\n",
        "\n",
        "if REMOVE_CHECKPOINTS_AND_MODEL_AND_RETRAIN:  # hadie\n",
        "    plt.plot(loss_plot)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Plot')\n",
        "    plt.savefig(\"trained_model_\" + feature_extraction_model + \"/learning_curve.png\")  # hadie\n",
        "    # plt.show() # hadie\n",
        "\n",
        "print(\"The learning curve has been written to trained_model_\" + feature_extraction_model + \"/learning_curve.png\")  # hadie\n",
        "print(\"Feature extraction model: \" + feature_extraction_model)\n",
        "print(\"Dataset: \" + DATASET)\n",
        "print(\"Development set proportion: \" + str(TEST_SET_PROPORTION))\n",
        "print(\"with yolo bounding boxes\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_folder = \"/content/drive/MyDrive/image_captioning/datasets/coco2014/val2014\"  # or test2014 if you have it\n",
        "\n",
        "\n",
        "img_name_test = sorted([\n",
        "    os.path.join(test_folder, f)\n",
        "    for f in os.listdir(test_folder)\n",
        "    if f.lower().endswith(\".jpg\")\n",
        "])\n",
        "\n",
        "# ‚úÖ 3. Filter only those with extracted .npy features\n",
        "img_name_test = [\n",
        "    path for path in img_name_test\n",
        "    if os.path.exists(path + \"_\" + feature_extraction_model + \".npy\")\n",
        "]"
      ],
      "metadata": {
        "id": "ZndobpCmXVWm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "results = []\n",
        "\n",
        "# Directory where results.json should be saved\n",
        "results_path = os.path.join(save_dir, \"results.json\")\n",
        "\n",
        "# ‚úÖ Ensure you have a loaded model (encoder + decoder)\n",
        "encoder.load_weights(os.path.join(save_dir, \"encoder.weights.h5\"))\n",
        "decoder.load_weights(os.path.join(save_dir, \"decoder.weights.h5\"))\n",
        "\n",
        "def evaluate_image(image_path):\n",
        "    \"\"\"Generate a caption for one image.\"\"\"\n",
        "    # Load precomputed features\n",
        "    img_tensor = np.load(image_path + \"_\" + feature_extraction_model + \".npy\")\n",
        "    img_tensor = tf.convert_to_tensor(img_tensor)\n",
        "    img_tensor = tf.expand_dims(img_tensor, 0)\n",
        "\n",
        "    features = encoder(img_tensor)\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(40):  # max caption length\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        word = tokenizer.index_word.get(predicted_id, '')\n",
        "        if word == '<end>':\n",
        "            break\n",
        "        result.append(word)\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "# ‚úÖ Evaluate a subset or all test images\n",
        "test_images = img_name_test[:50]  # or full test set\n",
        "for img_path in tqdm(test_images):\n",
        "    if not os.path.exists(img_path + \"_\" + feature_extraction_model + \".npy\"):\n",
        "        continue\n",
        "    predicted_caption = evaluate_image(img_path)\n",
        "    results.append({\n",
        "        \"image_id\": os.path.basename(img_path),\n",
        "        \"predicted_caption\": predicted_caption\n",
        "    })\n",
        "\n",
        "# ‚úÖ Save results to Drive\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Saved {len(results)} predictions to {results_path}\")\n"
      ],
      "metadata": {
        "id": "qASZr5LZXJ2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b193bde-3e2f-48b4-9ca2-3e1f83500f45"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:47<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 50 predictions to trained_model_xception/results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load COCO annotations\n",
        "with open('/content/drive/MyDrive/image_captioning/datasets/coco2014/annotations/captions_val2014.json') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "results = []\n",
        "\n",
        "# Evaluate on a small subset first (say 500 images for speed)\n",
        "for ann in tqdm(val_data['annotations'][:500]):\n",
        "    img_id = ann['image_id']\n",
        "    img_path = f\"/content/drive/MyDrive/image_captioning/datasets/coco2014/val2014/COCO_val2014_{img_id:012d}.jpg\"\n",
        "\n",
        "    # Get predicted caption\n",
        "    pred_caption, _ = evaluate(img_path)\n",
        "    pred_caption = ' '.join(pred_caption).replace('<start>', '').replace('<end>', '').strip()\n",
        "\n",
        "    results.append({\n",
        "        \"image_id\": img_id,\n",
        "        \"caption\": pred_caption\n",
        "    })\n",
        "\n",
        "# Save predictions in COCO format\n",
        "with open(\"results.json\", \"w\") as f:\n",
        "    json.dump(results, f)\n"
      ],
      "metadata": {
        "id": "oObfWDgpMapq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6f6d97-5770-46a3-f3b4-84126e606412"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [05:30<00:00,  1.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results[:5])"
      ],
      "metadata": {
        "id": "BlEn0Uw7SR1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91006885-faf7-4aaf-b212-c0ceb0c4db82"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'image_id': 203564, 'caption': 'a man sitting on a bike'}, {'image_id': 179765, 'caption': 'a tree parked in a parked in a city'}, {'image_id': 322141, 'caption': 'a bunch of an image of a bathroom toilet and a window as the wall sink in a mirror'}, {'image_id': 16977, 'caption': 'a patio view of a traffic traffic light no parking meter'}, {'image_id': 106140, 'caption': 'a icelandair airliner is getting ready to take off'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y openjdk-11-jdk\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTXPa1H89sxk",
        "outputId": "4561b2cf-32a5-49c8-ee65-dcc89917e778"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk is already the newest version (11.0.28+6-1ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-1k109bxk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-1k109bxk\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pycocoevalcap==1.2) (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
        "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')"
      ],
      "metadata": {
        "id": "pG-o1Od--9fv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "path = \"/content/drive/MyDrive/image_captioning/results.json\"\n",
        "\n",
        "# Load your predictions list\n",
        "with open(path, \"r\") as f:\n",
        "    preds = json.load(f)\n",
        "\n",
        "# Handle both list and dict formats safely\n",
        "if isinstance(preds, dict) and \"annotations\" in preds:\n",
        "    preds = preds[\"annotations\"]\n",
        "\n",
        "# Keep only one caption per image_id (e.g., the first occurrence)\n",
        "unique_preds = {}\n",
        "for p in preds:\n",
        "    if p[\"image_id\"] not in unique_preds:\n",
        "        unique_preds[p[\"image_id\"]] = p[\"caption\"]\n",
        "\n",
        "# Build clean JSON\n",
        "clean_preds = {\"annotations\": [{\"image_id\": k, \"caption\": v} for k, v in unique_preds.items()]}\n",
        "\n",
        "# Save to new file\n",
        "clean_path = \"/content/drive/MyDrive/image_captioning/results_clean.json\"\n",
        "with open(clean_path, \"w\") as f:\n",
        "    json.dump(clean_preds, f)\n",
        "\n",
        "print(f\"‚úÖ Cleaned predictions saved to {clean_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf-N1hGI_ch7",
        "outputId": "e5272467-586a-47f7-ec94-328ce31d452d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned predictions saved to /content/drive/MyDrive/image_captioning/results_clean.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "path = \"/content/drive/MyDrive/image_captioning/results.json\"\n",
        "\n",
        "# Load original predictions (list or dict)\n",
        "with open(path, \"r\") as f:\n",
        "    preds = json.load(f)\n",
        "\n",
        "# Handle both formats\n",
        "if isinstance(preds, dict) and \"annotations\" in preds:\n",
        "    preds = preds[\"annotations\"]\n",
        "\n",
        "# Keep one caption per image_id\n",
        "unique_preds = {}\n",
        "for p in preds:\n",
        "    if p[\"image_id\"] not in unique_preds:\n",
        "        unique_preds[p[\"image_id\"]] = p[\"caption\"]\n",
        "\n",
        "# ‚úÖ Save as a plain list\n",
        "clean_preds = [{\"image_id\": k, \"caption\": v} for k, v in unique_preds.items()]\n",
        "\n",
        "clean_path = \"/content/drive/MyDrive/image_captioning/results_clean.json\"\n",
        "with open(clean_path, \"w\") as f:\n",
        "    json.dump(clean_preds, f)\n",
        "\n",
        "print(f\"‚úÖ Cleaned predictions saved to {clean_path} ({len(clean_preds)} captions)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnwFSw9H_-2S",
        "outputId": "1e24babe-200c-4df6-9db0-bd297d388376"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned predictions saved to /content/drive/MyDrive/image_captioning/results_clean.json (143 captions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "annFile = '/content/drive/MyDrive/image_captioning/datasets/coco2014/annotations/captions_val2014.json'\n",
        "resFile = \"/content/drive/MyDrive/image_captioning/results_clean.json\"\n",
        "\n",
        "# Load both\n",
        "with open(annFile, \"r\") as f:\n",
        "    refs = json.load(f)[\"annotations\"]\n",
        "\n",
        "with open(resFile, \"r\") as f:\n",
        "    preds = json.load(f)\n",
        "\n",
        "# Collect sets of IDs\n",
        "ref_ids = {a[\"image_id\"] for a in refs}\n",
        "pred_ids = {p[\"image_id\"] for p in preds}\n",
        "\n",
        "# Check differences\n",
        "print(\"Total refs:\", len(ref_ids))\n",
        "print(\"Total preds:\", len(pred_ids))\n",
        "print(\"Missing in preds:\", len(ref_ids - pred_ids))\n",
        "print(\"Extra in preds:\", len(pred_ids - ref_ids))\n",
        "\n",
        "# Align to intersection only\n",
        "common_ids = ref_ids & pred_ids\n",
        "print(\"Common IDs:\", len(common_ids))\n",
        "\n",
        "# Filter to only matching IDs\n",
        "aligned_refs = [r for r in refs if r[\"image_id\"] in common_ids]\n",
        "aligned_preds = [p for p in preds if p[\"image_id\"] in common_ids]\n",
        "\n",
        "# Save aligned versions\n",
        "import os\n",
        "aligned_ref_path = \"/content/drive/MyDrive/image_captioning/references_aligned.json\"\n",
        "aligned_pred_path = \"/content/drive/MyDrive/image_captioning/results_aligned.json\"\n",
        "\n",
        "json.dump({\"annotations\": aligned_refs}, open(aligned_ref_path, \"w\"))\n",
        "json.dump(aligned_preds, open(aligned_pred_path, \"w\"))\n",
        "\n",
        "print(\"‚úÖ Saved aligned files:\")\n",
        "print(aligned_ref_path)\n",
        "print(aligned_pred_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqGUaeuyAP9V",
        "outputId": "79853820-fee5-4511-a9ef-d0778be52d0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total refs: 40504\n",
            "Total preds: 143\n",
            "Missing in preds: 40361\n",
            "Extra in preds: 0\n",
            "Common IDs: 143\n",
            "‚úÖ Saved aligned files:\n",
            "/content/drive/MyDrive/image_captioning/references_aligned.json\n",
            "/content/drive/MyDrive/image_captioning/results_aligned.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "\n",
        "coco = COCO(aligned_ref_path)\n",
        "cocoRes = coco.loadRes(aligned_pred_path)\n",
        "\n",
        "cocoEval = COCOEvalCap(coco, cocoRes)\n",
        "cocoEval.evaluate()\n",
        "\n",
        "for metric, score in cocoEval.eval.items():\n",
        "    print(f\"{metric}: {score:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "XFOcc5h-AVuF",
        "outputId": "1e03a474-1a7c-437d-8d88-71ad654510d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'info'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-775228723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_ref_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcocoRes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_pred_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcocoEval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCOEvalCap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcocoRes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36mloadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'info'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Paths\n",
        "aligned_ref_path = \"/content/drive/MyDrive/image_captioning/references_aligned.json\"\n",
        "aligned_pred_path = \"/content/drive/MyDrive/image_captioning/results_aligned.json\"\n",
        "\n",
        "# Load aligned refs\n",
        "with open(aligned_ref_path, \"r\") as f:\n",
        "    ref_data = json.load(f)\n",
        "\n",
        "# ref_data currently has only {'annotations': [...]}\n",
        "annotations = ref_data[\"annotations\"]\n",
        "\n",
        "# Build a minimal valid COCO structure\n",
        "images = [{\"id\": ann[\"image_id\"]} for ann in annotations]\n",
        "images = [dict(t) for t in {tuple(d.items()) for d in images}]  # remove duplicates\n",
        "\n",
        "ref_fixed = {\n",
        "    \"info\": {\"description\": \"Aligned references\"},\n",
        "    \"licenses\": [],\n",
        "    \"images\": images,\n",
        "    \"annotations\": annotations\n",
        "}\n",
        "\n",
        "fixed_ref_path = \"/content/drive/MyDrive/image_captioning/references_final.json\"\n",
        "json.dump(ref_fixed, open(fixed_ref_path, \"w\"))\n",
        "\n",
        "print(f\"‚úÖ Fixed references file saved to: {fixed_ref_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OGeaiK9AmFj",
        "outputId": "83452a93-81dd-4a23-caa2-c1be7e1f7441"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fixed references file saved to: /content/drive/MyDrive/image_captioning/references_final.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "\n",
        "annFile = \"/content/drive/MyDrive/image_captioning/references_final.json\"\n",
        "resFile = \"/content/drive/MyDrive/image_captioning/results_aligned.json\"\n",
        "\n",
        "coco = COCO(annFile)\n",
        "cocoRes = coco.loadRes(resFile)\n",
        "\n",
        "cocoEval = COCOEvalCap(coco, cocoRes)\n",
        "cocoEval.evaluate()\n",
        "\n",
        "for metric, score in cocoEval.eval.items():\n",
        "    print(f\"{metric}: {score:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAIuMpZBAoRI",
        "outputId": "d8aa76f8-86d0-4b46-de7c-c222c622871a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "tokenization...\n",
            "setting up scorers...\n",
            "computing Bleu score...\n",
            "{'testlen': 1711, 'reflen': 1429, 'guess': [1711, 1568, 1425, 1282], 'correct': [683, 197, 53, 13]}\n",
            "ratio: 1.1973407977598338\n",
            "Bleu_1: 0.399\n",
            "Bleu_2: 0.224\n",
            "Bleu_3: 0.123\n",
            "Bleu_4: 0.066\n",
            "computing METEOR score...\n",
            "METEOR: 0.142\n",
            "computing Rouge score...\n",
            "ROUGE_L: 0.324\n",
            "computing CIDEr score...\n",
            "CIDEr: 0.248\n",
            "computing SPICE score...\n",
            "SPICE: 0.081\n",
            "Bleu_1: 0.399\n",
            "Bleu_2: 0.224\n",
            "Bleu_3: 0.123\n",
            "Bleu_4: 0.066\n",
            "METEOR: 0.142\n",
            "ROUGE_L: 0.324\n",
            "CIDEr: 0.248\n",
            "SPICE: 0.081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "metrics = {\n",
        "    \"Bleu_1\": 0.399,\n",
        "    \"Bleu_2\": 0.224,\n",
        "    \"Bleu_3\": 0.123,\n",
        "    \"Bleu_4\": 0.066,\n",
        "    \"METEOR\": 0.142,\n",
        "    \"ROUGE_L\": 0.324,\n",
        "    \"CIDEr\": 0.248,\n",
        "    \"SPICE\": 0.081\n",
        "}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/image_captioning/checkpoints/baseline_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "print(\"‚úÖ Baseline metrics saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e3vvR-zDL8_",
        "outputId": "406ef95c-6847-4c81-c57f-dc3350abff64"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Baseline metrics saved.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM8k0Q9cR+8S4ErkQffDPKi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}